from collections.abc import Callable, Sized
import math
import os
from typing import Any
import torch
from tqdm.auto import tqdm, trange

import h5py

import numpy as np
from torch.utils.data import Dataset

from ray_tools.base.parameter import NumericalParameter, RandomOutputParameter, RandomParameter, RayParameterContainer

from .data_tools import h5_to_dict


class RayDataset(Dataset):
    """
    PyTorch dataset for files generated by :class:`ray_tools.simulation.data_tools.RandomRayDatasetGenerator`.
    :param h5_files: list of all h5-files to be read from.
    :param sub_groups: list of all h5 fields and/or subgroups to return when calling :func:`__getitem__`.
        The format is like 'subgroup1/subgroup2/...'.
        Note: Avoid listing unnecessary fields and groups, as this will slow down loading.
    :param nested_groups: If False, the strings in ``sub_groups`` are used as key for the returned data samples.
        If True the returned data samples form a nested dictionary generated according to ``sub_groups``.
    :param transform: Optional PyTorch transform to be applied to each data sample.
    """

    def __init__(self,
                 h5_files: list[str],
                 sub_groups: list[str],
                 nested_groups: bool = False,
                 transform: Callable | None = None):
        # np-array may help to avoid a memory leak
        # https://github.com/pytorch/pytorch/issues/13246#issuecomment-905703662
        self.h5_files = np.array(h5_files, dtype=str)
        self.sub_groups = sub_groups
        self.nested_groups = nested_groups
        self.transform = transform

        # get all h5-file handles
        self.h5_files_obj = [h5py.File(f, "r", swmr=True, libver='latest') for f in self.h5_files]

        # map that yields the consecutive index based on (h5-file index, sample index within h5-file)
        self.get_idx = dict()
        # map that yields a tuple (h5-file index, sample index within h5-file) given a (consecutive) index
        self.get_identifier = dict()
        # lengths of data in each h5file
        self._n_samples = []
        idx_total = 0
        for idx_h5, h5_file_obj in enumerate(self.h5_files_obj):
            # get length of data in file and check if same for all keys
            self._n_samples.append(len(h5_file_obj))

            for idx_sample in h5_file_obj.keys():
                self.get_idx[(idx_h5, idx_sample)] = idx_total
                self.get_identifier[idx_total] = [idx_h5, idx_sample]
                idx_total += 1

        # np-array may help to avoid a memory leak
        # https://github.com/pytorch/pytorch/issues/13246#issuecomment-905703662
        self.get_identifier = np.array([self.get_identifier[idx] for idx in range(idx_total)], dtype=str)

        self._n_samples_total = sum(self._n_samples)

        for h5_file_obj in self.h5_files_obj:
            h5_file_obj.close()

    def __getitem__(self, idx: int, sub_groups: list[str] | None = None, nested_groups: bool | None = None) -> dict[str, Any]:
        """
        :param idx: Index if data sample to return.
        :param sub_groups: Optional overwrite of class attribute ``sub_groups``.
        :param nested_groups: Optional overwrite of class attribute ``nested_groups``.
        :return: Data sample, either nested or not depending on ``nested_groups``.
        """
        sub_groups = sub_groups if sub_groups is not None else self.sub_groups
        nested_groups = nested_groups if nested_groups is not None else self.nested_groups

        # retrieve correct sample in corresponding h5-file
        idx_h5, idx_sample = self.get_identifier[idx]
        h5_file_obj = h5py.File(self.h5_files[int(idx_h5)], "r", libver='latest')
        sample_grp: h5py.Group = h5_file_obj[idx_sample]

        data = {}
        for grp in sub_groups:
            if nested_groups:
                # create nested dictionary structure according to sub_groups
                grp_split = grp.split(sep='/')
                sub_dict = data
                for key in grp_split[:-1]:
                    if key not in sub_dict:
                        sub_dict[key] = {}
                    sub_dict = sub_dict[key]
                sub_dict[grp_split[-1]] = h5_to_dict(sample_grp[grp])
            else:
                # create dictionary with keys equal to sub_groups
                data[grp] = h5_to_dict(sample_grp[grp])

        h5_file_obj.close()

        return self.transform(data) if self.transform else data

    def __len__(self) -> int:
        return self._n_samples_total


class MemoryDataset(Dataset):
    """
    Loads a given PyTorch dataset into the memory.
    :param dataset: PyTorch dataset to be read from.
    :param load_len: Restricts number of loaded samples.
    :param kwargs: Optional keywords for dataset.__getitem__.
    """

    def __init__(self, dataset: Dataset, load_len: int | None = None, item_len: int | None = None, **kwargs):
        super().__init__()
        if load_len is not None:
            if isinstance(dataset, Sized) and load_len > len(dataset):
                raise ValueError("Loaded length needs to be smaller or equal to dataset length.")
        else:
            assert isinstance(dataset, Sized)
            load_len = len(dataset)

        self.load_len = load_len
        self.memory_dataset = [dataset.__getitem__(idx, **kwargs)[:item_len] for idx in trange(load_len)]

    def __getitem__(self, idx: int) -> Any:
        return self.memory_dataset[idx]

    def __len__(self) -> int:
        return self.load_len

class BalancedMemoryDataset(Dataset):
    def __init__(self, dataset: Dataset, load_len: int | None = None, min_n_rays: int = 1000, good_samples_per_bad: int = 3, subset=None, **kwargs):
        super().__init__()
        self.min_n_rays = min_n_rays
        self.good_samples_per_bad = good_samples_per_bad
        self.subset = subset
        if load_len is not None:
            if isinstance(dataset, Sized) and load_len > len(dataset):
                raise ValueError("Loaded length needs to be smaller or equal to dataset length.")
        else:
            assert isinstance(dataset, Sized)
            load_len = len(dataset)

        self.load_len = load_len
        mask = dataset.data_dict['n_rays/ImagePlane'] >= min_n_rays
        self.good = (dataset.data_dict['parameters'][mask], dataset.data_dict['histogram/ImagePlane'][mask])
        self.bad = (dataset.data_dict['parameters'][~mask], dataset.data_dict['histogram/ImagePlane'][~mask])
        del dataset
        if not len(self.good[0]) > 0 or not len(self.bad[0]) > 0:
            raise Exception("Make sure that there are good and bad samples in your dataset.")

    def __getitem__(self, idx: int) -> Any:
        if self.subset == 'good':
            return self.good[0][idx], self.good[1][idx]
        elif self.subset == 'bad':
            return self.bad[0][idx], self.bad[1][idx]

        if idx%(self.good_samples_per_bad+1) == 0:
            translated_idx = (idx//(self.good_samples_per_bad+1)) % len(self.bad[0])
            return self.bad[0][translated_idx], self.bad[1][translated_idx]
        else:
            translated_idx = (idx - 1 - idx//(self.good_samples_per_bad+1)) % len(self.good[0])
            return self.good[0][translated_idx], self.good[1][translated_idx]

    def __len__(self) -> int:
        if self.subset == 'good':
            return len(self.good[0])
        elif self.subset == 'bad':
            return len(self.bad[0])
        else:
            return max(len(self.bad[0])*(self.good_samples_per_bad+1), math.ceil(len(self.good[0]) / self.good_samples_per_bad * (self.good_samples_per_bad+1)))

def extract_field(dataset: RayDataset, field: str) -> list[Any]:
    data = len(dataset) * [None]
    for idx in tqdm(range(len(dataset))):
        data[idx] = list(dataset.__getitem__(idx, sub_groups=[field], nested_groups=False).values())[0]
    return data

class HistDataset(Dataset):
    def __init__(self,
                 ids: list[int],
                 path: str,
                 file_pattern: str,
                 sub_groups: list[str],
                 transforms: list[Callable] | None = None,
                normalize_sub_groups: list[str]| None = None,
                load_max: int | None = None):
        file_pattern = file_pattern.split('*')
        h5_files = [os.path.join(path, file_pattern[0]+str(i)+file_pattern[1]) for i in ids]
        self.h5_files = np.array(h5_files, dtype=str)
        self.h5_files_obj = [h5py.File(f, "r", swmr=True, libver='latest') for f in self.h5_files]
        
        self.sub_groups = sub_groups
        self.transforms = transforms
        self.data_dict = dict([(key,[]) for key in sub_groups])
        
        
        for f in tqdm(self.h5_files_obj, leave=False):
            for key, value in self.data_dict.items():
                value.append(f[key][:load_max])
        for i, key in enumerate(self.sub_groups):
            data = np.concatenate(self.data_dict[key])
            if normalize_sub_groups is not None and key in normalize_sub_groups:
                min_list = []
                max_list = []
                for value in f[key].attrs.values():
                    if isinstance(value, np.ndarray):
                        min_list.append(value[0])
                        max_list.append(value[1])
                    else:
                        min_list.append(0.)
                        max_list.append(1.)
                min_vec = np.array(min_list)
                max_vec = np.array(max_list)
                data = (data - min_vec) / (max_vec - min_vec)
            assert self.transforms is None or len(self.transforms) == len(self.data_dict)
            value = torch.from_numpy(data)
            if self.transforms == None:
                self.data_dict[key] = value
            else:
                self.data_dict[key] = self.transforms[i](value)
                
    def __getitem__(self, idx: int):
        output = tuple([value[idx] for i, value in enumerate(self.data_dict.values())])
        return output

    def __len__(self) -> int:
        return self.data_dict[self.sub_groups[0]].shape[0]

    def retrieve_parameter_container(self):
        with h5py.File(self.h5_files[0], 'r') as f:
            exported_planes = f['histogram'].keys()
            
            param_container_list = []
            for key, value in f['parameters'].attrs.items():
                if hasattr(value, '__iter__'):
                    if key.split(".")[0] in exported_planes:
                        param_container_list.append((key, RandomOutputParameter(value_lims=value)))
                    else:
                        param_container_list.append((key, RandomParameter(value_lims=value)))
                else:
                    param_container_list.append((key, NumericalParameter(value)))
                    
            pc = RayParameterContainer(param_container_list)
            return pc

    def retrieve_xy_lims(self):
        with h5py.File(self.h5_files[0], 'r') as f:
            return f['histogram']['ImagePlane'].attrs['lims']
