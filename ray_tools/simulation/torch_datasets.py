from collections.abc import Callable, Sized
import math
from typing import Any
import torch
from tqdm import tqdm, trange

import h5py

import numpy as np
from torch.utils.data import Dataset

from ray_tools.base.parameter import NumericalParameter, RandomOutputParameter, RandomParameter, RayParameterContainer

from .data_tools import h5_to_dict


class RayDataset(Dataset):
    """
    PyTorch dataset for files generated by :class:`ray_tools.simulation.data_tools.RandomRayDatasetGenerator`.
    :param h5_files: list of all h5-files to be read from.
    :param sub_groups: list of all h5 fields and/or subgroups to return when calling :func:`__getitem__`.
        The format is like 'subgroup1/subgroup2/...'.
        Note: Avoid listing unnecessary fields and groups, as this will slow down loading.
    :param nested_groups: If False, the strings in ``sub_groups`` are used as key for the returned data samples.
        If True the returned data samples form a nested dictionary generated according to ``sub_groups``.
    :param transform: Optional PyTorch transform to be applied to each data sample.
    """

    def __init__(self,
                 h5_files: list[str],
                 sub_groups: list[str],
                 nested_groups: bool = False,
                 transform: Callable | None = None):
        # np-array may help to avoid a memory leak
        # https://github.com/pytorch/pytorch/issues/13246#issuecomment-905703662
        self.h5_files = np.array(h5_files, dtype=str)
        self.sub_groups = sub_groups
        self.nested_groups = nested_groups
        self.transform = transform

        # get all h5-file handles
        self.h5_files_obj = [h5py.File(f, "r", swmr=True, libver='latest') for f in self.h5_files]

        # map that yields the consecutive index based on (h5-file index, sample index within h5-file)
        self.get_idx = dict()
        # map that yields a tuple (h5-file index, sample index within h5-file) given a (consecutive) index
        self.get_identifier = dict()
        # lengths of data in each h5file
        self._n_samples = []
        idx_total = 0
        for idx_h5, h5_file_obj in enumerate(self.h5_files_obj):
            # get length of data in file and check if same for all keys
            self._n_samples.append(len(h5_file_obj))

            for idx_sample in h5_file_obj.keys():
                self.get_idx[(idx_h5, idx_sample)] = idx_total
                self.get_identifier[idx_total] = [idx_h5, idx_sample]
                idx_total += 1

        # np-array may help to avoid a memory leak
        # https://github.com/pytorch/pytorch/issues/13246#issuecomment-905703662
        self.get_identifier = np.array([self.get_identifier[idx] for idx in range(idx_total)], dtype=str)

        self._n_samples_total = sum(self._n_samples)

        for h5_file_obj in self.h5_files_obj:
            h5_file_obj.close()

    def __getitem__(self, idx: int, sub_groups: list[str] | None = None, nested_groups: bool | None = None) -> dict[str, Any]:
        """
        :param idx: Index if data sample to return.
        :param sub_groups: Optional overwrite of class attribute ``sub_groups``.
        :param nested_groups: Optional overwrite of class attribute ``nested_groups``.
        :return: Data sample, either nested or not depending on ``nested_groups``.
        """
        sub_groups = sub_groups if sub_groups is not None else self.sub_groups
        nested_groups = nested_groups if nested_groups is not None else self.nested_groups

        # retrieve correct sample in corresponding h5-file
        idx_h5, idx_sample = self.get_identifier[idx]
        h5_file_obj = h5py.File(self.h5_files[int(idx_h5)], "r", libver='latest')
        sample_grp: h5py.Group = h5_file_obj[idx_sample]

        data = {}
        for grp in sub_groups:
            if nested_groups:
                # create nested dictionary structure according to sub_groups
                grp_split = grp.split(sep='/')
                sub_dict = data
                for key in grp_split[:-1]:
                    if key not in sub_dict:
                        sub_dict[key] = {}
                    sub_dict = sub_dict[key]
                sub_dict[grp_split[-1]] = h5_to_dict(sample_grp[grp])
            else:
                # create dictionary with keys equal to sub_groups
                data[grp] = h5_to_dict(sample_grp[grp])

        h5_file_obj.close()

        return self.transform(data) if self.transform else data

    def __len__(self) -> int:
        return self._n_samples_total


class MemoryDataset(Dataset):
    """
    Loads a given PyTorch dataset into the memory.
    :param dataset: PyTorch dataset to be read from.
    :param load_len: Restricts number of loaded samples.
    :param kwargs: Optional keywords for dataset.__getitem__.
    """

    def __init__(self, dataset: Dataset, load_len: int | None = None, **kwargs):
        super().__init__()
        if load_len is not None:
            if isinstance(dataset, Sized) and load_len > len(dataset):
                raise ValueError("Loaded length needs to be smaller or equal to dataset length.")
        else:
            assert isinstance(dataset, Sized)
            load_len = len(dataset)

        self.load_len = load_len
        self.memory_dataset = [dataset.__getitem__(idx, **kwargs) for idx in trange(load_len)]

    def __getitem__(self, idx: int) -> Any:
        return self.memory_dataset[idx]

    def __len__(self) -> int:
        return self.load_len

class BalancedMemoryDataset(Dataset):
    def __init__(self, dataset: Dataset, load_len: int | None = None, min_n_rays: int = 1000, good_samples_per_bad: int = 3, debug_mode=False, **kwargs):
        super().__init__()
        self.min_n_rays = min_n_rays
        self.good_samples_per_bad = good_samples_per_bad
        if load_len is not None:
            if isinstance(dataset, Sized) and load_len > len(dataset):
                raise ValueError("Loaded length needs to be smaller or equal to dataset length.")
        else:
            assert isinstance(dataset, Sized)
            load_len = len(dataset)

        self.load_len = load_len
        self.good = []
        self.bad = []
        for idx in trange(load_len):
            new_item = dataset.__getitem__(idx, **kwargs)
            if new_item[2] >= min_n_rays or (debug_mode and idx % 2 == 0):
                self.good.append(new_item[:2])
            else:
                self.bad.append(new_item[:2])
        if not len(self.good) > 0 or not len(self.bad) > 0:
            raise Exception("Make sure that there are good and bad samples in your dataset.")

    def __getitem__(self, idx: int) -> Any:
        if idx%(self.good_samples_per_bad+1) == 0:
            return self.bad[(idx//(self.good_samples_per_bad+1)) % len(self.bad)]
        else:
            return self.good[(idx - 1 - idx//(self.good_samples_per_bad+1)) % len(self.good)]

    def __len__(self) -> int:
        return max(len(self.bad)*(self.good_samples_per_bad+1), math.ceil(len(self.good) / self.good_samples_per_bad * (self.good_samples_per_bad+1)))

def extract_field(dataset: RayDataset, field: str) -> list[Any]:
    data = len(dataset) * [None]
    for idx in tqdm(range(len(dataset))):
        data[idx] = list(dataset.__getitem__(idx, sub_groups=[field], nested_groups=False).values())[0]
    return data

class HistDataset(Dataset):
    def __init__(self,
                 h5_files: list[str],
                 sub_groups: list[str],
                 transforms: list[Callable] | None = None,
                normalize_sub_groups: list[str]| None = None,
                load_max: int | None = None):
        self.h5_files = np.array(h5_files, dtype=str)
        self.h5_files_obj = [h5py.File(f, "r", swmr=True, libver='latest') for f in self.h5_files]
        
        self.sub_groups = sub_groups
        self.transforms = transforms
        self.data_dict = dict([(key,[]) for key in sub_groups])
        
        
        for f in self.h5_files_obj:
            for key, value in self.data_dict.items():
                value.append(f[key][:load_max])
        for key in self.sub_groups:
            data = np.concatenate(self.data_dict[key])
            if normalize_sub_groups is not None and key in normalize_sub_groups:
                min_list = []
                max_list = []
                for value in f[key].attrs.values():
                    if isinstance(value, np.ndarray):
                        min_list.append(value[0])
                        max_list.append(value[1])
                    else:
                        min_list.append(0.)
                        max_list.append(1.)
                min_vec = np.array(min_list)
                max_vec = np.array(max_list)
                data = (data - min_vec) / (max_vec - min_vec)
            self.data_dict[key] = torch.from_numpy(data)
    def __getitem__(self, idx: int):
        assert self.transforms is None or len(self.transforms) == len(self.data_dict)
        return tuple([self.transforms[i](value[idx]) if self.transforms is not None else value[idx] for i, value in enumerate(self.data_dict.values())])
    def __len__(self) -> int:
        return self.data_dict[self.sub_groups[0]].shape[0]
    
    @staticmethod
    def retrieve_parameter_container(h5_file_path):
        with h5py.File(h5_file_path, 'r') as f:
            exported_planes = f['histogram'].keys()
            
            param_container_list = []
            for key, value in f['parameters'].attrs.items():
                if hasattr(value, '__iter__'):
                    if key.split(".")[0] in exported_planes:
                        param_container_list.append((key, RandomOutputParameter(value_lims=value)))
                    else:
                        param_container_list.append((key, RandomParameter(value_lims=value)))
                else:
                    param_container_list.append((key, NumericalParameter(value)))
                    
            pc = RayParameterContainer(param_container_list)
            return pc

    @staticmethod
    def retrieve_xy_lims(h5_file_path):
        with h5py.File(h5_file_path, 'r') as f:
            return f['histogram']['ImagePlane'].attrs['lims']
