{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab8113-e0e8-44e6-a38c-a7889f640d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "from tqdm.auto import tqdm, trange\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "import umap\n",
    "from torch.utils import benchmark\n",
    "from hist_optimizer import *\n",
    "\n",
    "from ray_tools.base.transform import MultiLayer\n",
    "from ray_tools.base.engine import RayEngine\n",
    "from ray_nn.data.lightning_data_module import DefaultDataModule\n",
    "from ray_nn.nn.xy_hist_data_models import MetrixXYHistSurrogate, StandardizeXYHist, HistSurrogateEngine, Model\n",
    "from ray_tools.base.backend import RayBackendDockerRAYUI\n",
    "from ray_tools.simulation.torch_datasets import BalancedMemoryDataset, MemoryDataset, HistDataset\n",
    "from ray_optim.plot import Plot\n",
    "from ray_tools.base.transform import Histogram, RayTransformConcat, XYHistogram\n",
    "from ray_nn.data.transform import Select\n",
    "from ray_tools.base.parameter import NumericalParameter, OutputParameter, NumericalOutputParameter, MutableParameter, RayParameterContainer\n",
    "from sub_projects.ray_optimization.real_data import import_data\n",
    "from sub_projects.ray_optimization.utils import ray_dict_to_tensor, ray_output_to_tensor\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe67c8a6-0a58-49d5-8c68-a6c129ca6eb2",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b643820-f6e9-49cb-97e2-6a96f940fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '../../'\n",
    "engine = RayEngine(rml_basefile=os.path.join(root_dir, 'rml_src/METRIX_U41_G1_H1_318eV_PS_MLearn_1.15.rml'),\n",
    "                                exported_planes=[\"ImagePlane\"],\n",
    "                                ray_backend=RayBackendDockerRAYUI(docker_image='ray-ui-service',\n",
    "                                                                  docker_container_name='ray-ui-service-test',\n",
    "                                                                  dockerfile_path='../../ray_docker/rayui',\n",
    "                                                                  ray_workdir='/dev/shm/ray-workdir',\n",
    "                                                                  verbose=False),\n",
    "                                num_workers=-1,\n",
    "                                as_generator=False)\n",
    "\n",
    "\n",
    "model_path = os.path.join(root_dir, \"outputs/xy_hist/ft1rr9h0/checkpoints/epoch=68-step=65665644.ckpt\")\n",
    "surrogate_engine = HistSurrogateEngine(checkpoint_path=model_path)\n",
    "\n",
    "model = Model(path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a50f86d-fcb8-493a-afea-27db01664f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from ipywidgets import interact\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(X, Y, Z):\n",
    "    inp = torch.ones((1,model.mutable_parameter_count), device=model.device)*0.5\n",
    "    inp[:,-3] = X\n",
    "    inp[:,-2] = Y\n",
    "    inp[:,-4] = Z\n",
    "    ##out = model(inp)\n",
    "    out = surrogate_engine.run([tensor_to_param_container(inp[0], model.input_parameter_container)])\n",
    "    #out =  engine.run([tensor_to_param_container(inp[0], model.input_parameter_container)])\n",
    "    #out = [XYHistogram(50, (-10., 10.), (-3., 3.))(out[0]['ray_output']['ImagePlane'])['histogram'].flatten()]\n",
    "    out = [torch.cat((out[0]['ray_output']['ImagePlane']['xy_hist'].x_loc, out[0]['ray_output']['ImagePlane']['xy_hist'].y_loc))]\n",
    "    return out[0].detach().cpu().numpy()#A*x**2 + B*x + Z\n",
    "\n",
    "#fig = plt.figure()\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "histogram_lims = model.histogram_lims\n",
    "lineX, = ax[0].plot(np.linspace(histogram_lims[0][0], histogram_lims[0][1], num=50), f(X=.5, Y=.5, Z=.0)[:50])\n",
    "lineY, = ax[1].plot(np.linspace(histogram_lims[1][0], histogram_lims[1][1], num=50), f(X=.5, Y=.5, Z=.0)[50:])\n",
    "\n",
    "def update(X = .5, Y = .5, Z = .0):\n",
    "    new_f = f(X,Y,Z)\n",
    "    new_x = new_f[:50]\n",
    "    lineX.set_ydata(new_x)\n",
    "    ax[0].set_ylim(new_x.min(), new_x.max())\n",
    "    new_y = new_f[50:]\n",
    "    lineY.set_ydata(new_y)\n",
    "    ax[1].set_ylim(new_y.min(), new_y.max())\n",
    "    fig.canvas.draw_idle()\n",
    "    \n",
    "interact(update, X = (0,1,0.05), Y = (0,1,0.05), Z = (0,1,0.1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a6bb54-8d38-4597-9756-0aa567b8b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_list = []\n",
    "for i in range(20):\n",
    "    inp = torch.ones(model.mutable_parameter_count, device=model.device)*0.5\n",
    "    #inp[-3] = 0.5# + i*0.5\n",
    "    inp[-4] = 0. + i*0.05\n",
    "    inp_list.append(inp)\n",
    "\n",
    "#Plot.plot_engines_comparison(engine, surrogate_engine, [tensor_to_param_container(inp, model.input_parameter_container) for inp in inp_list], MultiLayer([0.]))\n",
    "inp = torch.stack(inp_list)\n",
    "plot_param_tensors( inp, inp, engine = engine, ray_parameter_container=model.input_parameter_container, compensated_parameters = inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435f5fe1-0fd6-4768-bfca-6a04886402bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(inp)\n",
    "plotted, _ = out.max(dim=-1)\n",
    "plt.plot(plotted.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19d0003-49c6-4b29-8ae4-3a0e3eadc779",
   "metadata": {},
   "source": [
    "# Look with model for new sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970756bd-471c-4fac-a271-ee04c7adaf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets_selected, uncompensated_parameters_selected, compensated_parameters_selected = find_good_offset_problem(model, fixed_parameters = [8, 14, 20, 21, 27, 28, 34])\n",
    "\n",
    "with torch.no_grad():\n",
    "    observed_rays = model(compensated_parameters_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eca460-d26c-4d90-90e3-c73027e1af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets_list = []\n",
    "uncompensated_parameters_list = []\n",
    "compensated_parameters_list = []\n",
    "for i in range(5000):\n",
    "    offsets_selected, uncompensated_parameters_selected, compensated_parameters_selected = find_good_offset_problem(model, fixed_parameters = [8, 14, 20, 21, 27, 28, 34])\n",
    "    offsets_list.append(offsets_selected)\n",
    "    uncompensated_parameters_list.append(uncompensated_parameters_selected)\n",
    "    compensated_parameters_list.append(compensated_parameters_selected)\n",
    "\n",
    "offsets_list = torch.stack(offsets_list)\n",
    "print(offsets_selected.mean().item(), \"Â±\", offsets_selected.std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5067f041-c0cb-4e60-a89a-700ce5cdb6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data = uncompensated_parameters_selected[:, 0, 0, :]\n",
    "fig, ax = plt.subplots(2,1, figsize=(32,8))\n",
    "\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "# Plot each line with a different color from the cycle\n",
    "for i, y in enumerate(uncompensated_parameters_list):\n",
    "    ax[0].plot(y[:, 10, 0].T.cpu(), color=colors[i % len(colors)], alpha=0.2)\n",
    "    ax[0].set_title(\"Uncompensated parameters\")\n",
    "    ax[0].set_xlabel(\"Beamline parameter [#]\")\n",
    "    ax[0].set_xticks(torch.arange(37))\n",
    "    ax[1].plot(compensated_parameters_list[i][:, 10, 0].T.cpu(), color=colors[i % len(colors)], alpha=0.2)\n",
    "    ax[1].set_title(\"Compensated parameters\")\n",
    "    ax[1].set_xlabel(\"Beamline parameter [#]\")\n",
    "    ax[1].set_xticks(torch.arange(37))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9910d1a-ef38-46bd-8a4c-f383c7484a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, sharey=True)\n",
    "for entry in offsets_list:\n",
    "    ax[0].scatter(entry[0].detach().cpu(), y=torch.arange(37).detach().cpu())\n",
    "    ax[0].set_ylabel(\"Beamline parameter [#]\")\n",
    "    ax[0].set_xlabel(\"Chosen offset\")\n",
    "    ax[0].set_title(\"Simulated real beamline\")\n",
    "for entry in offsets_list:\n",
    "    ax[1].scatter(torch.rand(37).detach().cpu(), y=torch.arange(37).detach().cpu())\n",
    "    ax[1].set_xlabel(\"Chosen offset\")\n",
    "    ax[1].set_title(\"Random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abb5e51-e9e8-4328-83f1-bec9927aadc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstest\n",
    "\n",
    "def check_if_uniform(check_list):\n",
    "    not_uniform_list = []\n",
    "    for i, entry in enumerate(check_list.T):\n",
    "        statistic, p_value = kstest(entry.cpu(), 'uniform', args=(0, 1))\n",
    "        # Interpret p-value\n",
    "        if p_value > 0.01:  # Common significance level is 0.05\n",
    "            #print(\"Fail to reject the null hypothesis: Data appears to be uniformly distributed.\")\n",
    "            continue\n",
    "        else:\n",
    "            print(i, \"Reject the null hypothesis: Data does not appear to be uniformly distributed.\")\n",
    "            not_uniform_list.append(i)\n",
    "    return not_uniform_list\n",
    "\n",
    "check_if_uniform(offsets_list.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba780a70-7a25-47df-ac97-6cab6f22c0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "mins, _ = uncompensated_parameters_selected[:, 0, 0, :].max(dim=0)\n",
    "print((uncompensated_parameters_selected[:, 0, 0, :] == mins).sum(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b475c34a-7816-4b23-aefb-e0263b68c228",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, entry in model.offset_space.items():\n",
    "    if isinstance(entry, MutableParameter):\n",
    "        value_list = [value.value_lims for label2, value in model.input_parameter_container.items() if isinstance(value, MutableParameter) and label==label2]\n",
    "        print(\"label:\", entry.value_lims, \"from\", value_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4619825c-6b7b-40f5-897b-abca776a6466",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_min_params, loss, loss_min_list = optimize_smart_walker(model, observed_rays, uncompensated_parameters_selected, iterations=1000, num_candidates=10000, step_width=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f9432e-9daa-4356-92d4-9e7120fa6bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.optimize import basinhopping\n",
    "\n",
    "def torch_to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy()\n",
    "\n",
    "def numpy_to_torch(array, device):\n",
    "    return torch.tensor(array, device=device, dtype=torch.float32)\n",
    "\n",
    "def objective_function(offsets_numpy, model, observed_rays, uncompensated_parameters):\n",
    "    # Convert numpy array to torch tensor\n",
    "    offsets = numpy_to_torch(offsets_numpy, device=model.device).unsqueeze(0)\n",
    "    #offsets = torch.clamp(offsets, 0, 1)\n",
    "    \n",
    "    # Evaluate the model's loss with these offsets\n",
    "    scaled_offsets = model.rescale_offset(offsets)\n",
    "    compensated_rays = model(uncompensated_parameters + scaled_offsets)\n",
    "    loss = ((compensated_rays - observed_rays) ** 2).mean().item()\n",
    "    return loss\n",
    "\n",
    "def optimize_smart_walker_scipy(model, observed_rays, uncompensated_parameters, iterations=100, hop_step=0.1, interval=1, stepsize=0.1):\n",
    "    # Convert initial parameters to numpy for scipy\n",
    "    initial_offsets = torch.rand(1, uncompensated_parameters.shape[-1], device=model.device)\n",
    "    initial_offsets_numpy = torch_to_numpy(initial_offsets).flatten()\n",
    "\n",
    "    # Define a list to track the minimum loss for each iteration\n",
    "    loss_min_list = []\n",
    "\n",
    "    # Set up the progress bar\n",
    "    pbar = tqdm(total=iterations)\n",
    "\n",
    "    def callback(x, f, accept):\n",
    "        # Update progress bar and log the loss\n",
    "        loss_min_list.append(f)\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix({\"loss\": f})\n",
    "\n",
    "    # Configure basinhopping with scipy\n",
    "    minimizer_kwargs = {\n",
    "        \"method\": \"Powell\",\n",
    "        \"args\": (model, observed_rays, uncompensated_parameters),\n",
    "        \"bounds\": [(0, 1)] * initial_offsets_numpy.size  # Bounds for each parameter\n",
    "    }\n",
    "    \n",
    "    # Run basinhopping\n",
    "    result = basinhopping(\n",
    "        objective_function, \n",
    "        initial_offsets_numpy, \n",
    "        niter=iterations,\n",
    "        T=hop_step,\n",
    "        stepsize=stepsize,\n",
    "        interval=interval,\n",
    "        minimizer_kwargs=minimizer_kwargs,\n",
    "        callback=callback,\n",
    "        disp=True\n",
    "    )\n",
    "\n",
    "    # Close the progress bar\n",
    "    pbar.close()\n",
    "    \n",
    "    # Convert the final result back to torch for further use\n",
    "    final_offsets_numpy = result.x\n",
    "    final_offsets_torch = numpy_to_torch(final_offsets_numpy, device=model.device).view(1, -1)\n",
    "    scaled_offsets = model.rescale_offset(final_offsets_torch.unsqueeze(1))\n",
    "    best_loss_params = uncompensated_parameters + scaled_offsets\n",
    "\n",
    "    return best_loss_params, result.fun, loss_min_list\n",
    "loss_min_params, loss, loss_min_list = optimize_smart_walker_scipy(model, observed_rays, uncompensated_parameters_selected, hop_step=0.1, iterations=1000, interval=10, stepsize=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380bf0f2-3bff-4f81-9fd4-63d8ed80d2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "optimize_tpe(model, observed_rays, uncompensated_parameters_selected, iterations=1000, num_candidates=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6538f0dc-1072-46c3-a16f-b9e54319dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.fit import fit_gpytorch_mll_torch\n",
    "from botorch.acquisition import qLogExpectedImprovement\n",
    "from botorch.acquisition.monte_carlo import qNoisyExpectedImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.utils.sampling import manual_seed\n",
    "from botorch.utils.transforms import unnormalize, normalize\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "def objective_function(offsets, model, observed_rays, uncompensated_parameters):\n",
    "    \"\"\"\n",
    "    Evaluate the objective function.\n",
    "    \"\"\"\n",
    "    # Scale offsets and compute compensated rays\n",
    "    scaled_offsets = model.rescale_offset(offsets)\n",
    "    compensated_rays = model(uncompensated_parameters + scaled_offsets)\n",
    "\n",
    "    # Calculate loss (mean squared error)\n",
    "    loss = ((compensated_rays - observed_rays) ** 2).mean().item()\n",
    "    return loss\n",
    "\n",
    "def optimize_smart_walker_botorch(\n",
    "    model, observed_rays, uncompensated_parameters, iterations=100, num_initial_points=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Optimize the smart walker using BoTorch with TS/qEI.\n",
    "    \"\"\"\n",
    "    device = model.device\n",
    "    dtype = torch.float\n",
    "\n",
    "    # Define parameter bounds\n",
    "    bounds = torch.tensor([[0.0] * uncompensated_parameters.shape[-1], \n",
    "                           [1.0] * uncompensated_parameters.shape[-1]], device=device, dtype=dtype)\n",
    "    dim = uncompensated_parameters.shape[-1]\n",
    "\n",
    "    # Generate random initial samples\n",
    "    train_X = torch.rand((num_initial_points, dim), device=device, dtype=dtype)\n",
    "    train_Y = torch.tensor(\n",
    "        [\n",
    "            [objective_function(x, model, observed_rays, uncompensated_parameters)]\n",
    "            for x in train_X\n",
    "        ],\n",
    "        device=device,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    # Start optimization\n",
    "    for i in range(iterations):\n",
    "        # Fit a GP model\n",
    "        gp = SingleTaskGP(train_X, train_Y)\n",
    "        mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "        fit_gpytorch_mll_torch(mll)\n",
    "\n",
    "        # Define the acquisition function (qEI)\n",
    "        qEI = qLogExpectedImprovement(model=gp, best_f=train_Y.min().item())\n",
    "\n",
    "        # Optimize the acquisition function to find the next set of candidates\n",
    "        candidates, _ = optimize_acqf(\n",
    "            acq_function=qEI,\n",
    "            bounds=bounds,\n",
    "            q=1,  # Optimize one point at a time\n",
    "            num_restarts=10,\n",
    "            raw_samples=100,\n",
    "        )\n",
    "\n",
    "        # Evaluate the objective at the new candidate\n",
    "        new_X = candidates.detach()\n",
    "        new_Y = torch.tensor(\n",
    "            [[objective_function(new_X[0], model, observed_rays, uncompensated_parameters)]],\n",
    "            device=device,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "\n",
    "        # Append new data to the training set\n",
    "        train_X = torch.cat([train_X, new_X], dim=0)\n",
    "        train_Y = torch.cat([train_Y, new_Y], dim=0)\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Iteration {i + 1}/{iterations}, Best Loss: {train_Y.min().item()}\")\n",
    "\n",
    "    # Return the best found parameters and their scaled offsets\n",
    "    best_offset_index = train_Y.argmin()\n",
    "    best_offsets = train_X[best_offset_index]\n",
    "    scaled_offsets = model.rescale_offset(best_offsets.unsqueeze(0))\n",
    "    loss_min_params = uncompensated_parameters + scaled_offsets\n",
    "    best_loss = train_Y.min().item()\n",
    "\n",
    "    return loss_min_params, best_loss, train_X, train_Y\n",
    "\n",
    "# Example usage\n",
    "loss_min_params, loss, train_X, train_Y = optimize_smart_walker_botorch(\n",
    "    model, observed_rays, uncompensated_parameters_selected, iterations=100, num_initial_points=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b244e20a-6c7b-4493-be29-90d101d4d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# coding: utf-8\n",
    "\n",
    "# ## BO with TuRBO-1 and TS/qEI\n",
    "# \n",
    "# In this tutorial, we show how to implement Trust Region Bayesian Optimization (TuRBO) [1] in a closed loop in BoTorch.\n",
    "# \n",
    "# This implementation uses one trust region (TuRBO-1) and supports either parallel expected improvement (qEI) or Thompson sampling (TS). We optimize the $20D$ Ackley function on the domain $[-5, 10]^{20}$ and show that TuRBO-1 outperforms qEI as well as Sobol.\n",
    "# \n",
    "# Since botorch assumes a maximization problem, we will attempt to maximize $-f(x)$ to achieve $\\max_x -f(x)=0$.\n",
    "# \n",
    "# [1]: [Eriksson, David, et al. Scalable global optimization via local Bayesian optimization. Advances in Neural Information Processing Systems. 2019](https://proceedings.neurips.cc/paper/2019/file/6c990b7aca7bc7058f5e98ea909e924b-Paper.pdf)\n",
    "# \n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import os\n",
    "import math\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from botorch.acquisition import qExpectedImprovement, qLogExpectedImprovement\n",
    "from botorch.exceptions import BadInitialCandidatesWarning\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.generation import MaxPosteriorSampling\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.test_functions import Ackley\n",
    "from botorch.utils.transforms import unnormalize\n",
    "from torch.quasirandom import SobolEngine\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.constraints import Interval\n",
    "from gpytorch.kernels import MaternKernel, ScaleKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=BadInitialCandidatesWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.double\n",
    "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\n",
    "\n",
    "\n",
    "# ## Optimize the 20-dimensional Ackley function\n",
    "# \n",
    "# The goal is to minimize the popular Ackley function:\n",
    "# \n",
    "# $f(x_1,\\ldots,x_d) = -20\\exp\\left(-0.2 \\sqrt{\\frac{1}{d} \\sum_{j=1}^d x_j^2} \\right) -\\exp \\left( \\frac{1}{d} \\sum_{j=1}^d \\cos(2 \\pi x_j) \\right) + 20 + e$\n",
    "# \n",
    "# over the domain  $[-5, 10]^{20}$.  The global optimal value of $0$ is attained at $x_1 = \\ldots = x_d = 0$.\n",
    "# \n",
    "# As mentioned above, since botorch assumes a maximization problem, we instead maximize $-f(x)$.\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "def objective_function(offsets, model, observed_rays, uncompensated_parameters):\n",
    "    \"\"\"\n",
    "    Evaluate the objective function.\n",
    "    \"\"\"\n",
    "    # Scale offsets and compute compensated rays\n",
    "    scaled_offsets = model.rescale_offset(offsets)\n",
    "    compensated_rays = model(uncompensated_parameters + scaled_offsets)\n",
    "\n",
    "    # Calculate loss (mean squared error)\n",
    "    loss = ((compensated_rays - observed_rays) ** 2).mean()\n",
    "    return loss\n",
    "\n",
    "def fun(offsets):\n",
    "    return -objective_function(offsets.float(), model, observed_rays, uncompensated_parameters_selected).double()\n",
    "    \n",
    "\n",
    "#fun = Ackley(dim=20, negate=True).to(dtype=dtype, device=device)\n",
    "#fun.bounds[0, :].fill_(0)\n",
    "#fun.bounds[1, :].fill_(1)\n",
    "dim = 37#fun.dim\n",
    "lb, ub = (0., 1.) #fun.bounds\n",
    "\n",
    "batch_size = 100\n",
    "n_init = 2 * dim\n",
    "max_cholesky_size = float(\"inf\")  # Always use Cholesky\n",
    "\n",
    "\n",
    "def eval_objective(x):\n",
    "    \"\"\"This is a helper function we use to unnormalize and evalaute a point\"\"\"\n",
    "    return fun(x) #fun(unnormalize(x, fun.bounds))\n",
    "\n",
    "\n",
    "# ## Maintain the TuRBO state\n",
    "# TuRBO needs to maintain a state, which includes the length of the trust region, success and failure counters, success and failure tolerance, etc. \n",
    "# \n",
    "# In this tutorial we store the state in a dataclass and update the state of TuRBO after each batch evaluation. \n",
    "# \n",
    "# **Note**: These settings assume that the domain has been scaled to $[0, 1]^d$ and that the same batch size is used for each iteration.\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TurboState:\n",
    "    dim: int\n",
    "    batch_size: int\n",
    "    length: float = 0.8\n",
    "    length_min: float = 0.5**7\n",
    "    length_max: float = 1.6\n",
    "    failure_counter: int = 0\n",
    "    failure_tolerance: int = float(\"nan\")  # Note: Post-initialized\n",
    "    success_counter: int = 0\n",
    "    success_tolerance: int = 10  # Note: The original paper uses 3\n",
    "    best_value: float = -float(\"inf\")\n",
    "    restart_triggered: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.failure_tolerance = math.ceil(\n",
    "            max([4.0 / self.batch_size, float(self.dim) / self.batch_size])\n",
    "        )\n",
    "\n",
    "\n",
    "def update_state(state, Y_next):\n",
    "    if max(Y_next) > state.best_value + 1e-3 * math.fabs(state.best_value):\n",
    "        state.success_counter += 1\n",
    "        state.failure_counter = 0\n",
    "    else:\n",
    "        state.success_counter = 0\n",
    "        state.failure_counter += 1\n",
    "\n",
    "    if state.success_counter == state.success_tolerance:  # Expand trust region\n",
    "        state.length = min(2.0 * state.length, state.length_max)\n",
    "        state.success_counter = 0\n",
    "    elif state.failure_counter == state.failure_tolerance:  # Shrink trust region\n",
    "        state.length /= 2.0\n",
    "        state.failure_counter = 0\n",
    "\n",
    "    state.best_value = max(state.best_value, max(Y_next).item())\n",
    "    if state.length < state.length_min:\n",
    "        state.restart_triggered = True\n",
    "    return state\n",
    "\n",
    "\n",
    "# ## Take a look at the state\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "state = TurboState(dim=dim, batch_size=batch_size)\n",
    "print(state)\n",
    "\n",
    "\n",
    "# ## Generate initial points\n",
    "# This generates an initial set of Sobol points that we use to start of the BO loop.\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "def get_initial_points(dim, n_pts, seed=0):\n",
    "    sobol = SobolEngine(dimension=dim, scramble=True, seed=seed)\n",
    "    X_init = sobol.draw(n=n_pts).to(dtype=dtype, device=device)\n",
    "    return X_init\n",
    "\n",
    "\n",
    "# ## Generate new batch\n",
    "# Given the current `state` and a probabilistic (GP) `gp_model` built from observations `X` and `Y`, we generate a new batch of points.  \n",
    "# \n",
    "# This method works on the domain $[0, 1]^d$, so make sure to not pass in observations from the true domain.  `unnormalize` is called before the true function is evaluated which will first map the points back to the original domain.\n",
    "# \n",
    "# We support either TS and qEI which can be specified via the `acqf` argument.\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "def generate_batch(\n",
    "    state,\n",
    "    model,  # GP model\n",
    "    X,  # Evaluated points on the domain [0, 1]^d\n",
    "    Y,  # Function values\n",
    "    batch_size,\n",
    "    n_candidates=None,  # Number of candidates for Thompson sampling\n",
    "    num_restarts=10,\n",
    "    raw_samples=512,\n",
    "    acqf=\"ts\",  # \"ei\" or \"ts\"\n",
    "):\n",
    "    assert acqf in (\"ts\", \"ei\")\n",
    "    assert X.min() >= 0.0 and X.max() <= 1.0 and torch.all(torch.isfinite(Y))\n",
    "    if n_candidates is None:\n",
    "        n_candidates = min(5000, max(2000, 200 * X.shape[-1]))\n",
    "\n",
    "    # Scale the TR to be proportional to the lengthscales\n",
    "    x_center = X[Y.argmax(), :].clone()\n",
    "    weights = gp_model.covar_module.base_kernel.lengthscale.squeeze().detach()\n",
    "    weights = weights / weights.mean()\n",
    "    weights = weights / torch.prod(weights.pow(1.0 / len(weights)))\n",
    "    tr_lb = torch.clamp(x_center - weights * state.length / 2.0, 0.0, 1.0)\n",
    "    tr_ub = torch.clamp(x_center + weights * state.length / 2.0, 0.0, 1.0)\n",
    "\n",
    "    if acqf == \"ts\":\n",
    "        dim = X.shape[-1]\n",
    "        sobol = SobolEngine(dim, scramble=True)\n",
    "        pert = sobol.draw(n_candidates).to(dtype=dtype, device=device)\n",
    "        pert = tr_lb + (tr_ub - tr_lb) * pert\n",
    "\n",
    "        # Create a perturbation mask\n",
    "        prob_perturb = min(20.0 / dim, 1.0)\n",
    "        mask = torch.rand(n_candidates, dim, dtype=dtype, device=device) <= prob_perturb\n",
    "        ind = torch.where(mask.sum(dim=1) == 0)[0]\n",
    "        mask[ind, torch.randint(0, dim - 1, size=(len(ind),), device=device)] = 1\n",
    "\n",
    "        # Create candidate points from the perturbations and the mask\n",
    "        X_cand = x_center.expand(n_candidates, dim).clone()\n",
    "        X_cand[mask] = pert[mask]\n",
    "\n",
    "        # Sample on the candidate points\n",
    "        thompson_sampling = MaxPosteriorSampling(model=gp_model, replacement=False)\n",
    "        with torch.no_grad():  # We don't need gradients when using TS\n",
    "            X_next = thompson_sampling(X_cand, num_samples=batch_size)\n",
    "\n",
    "    elif acqf == \"ei\":\n",
    "        ei = qExpectedImprovement(gp_model, train_Y.max())\n",
    "        X_next, acq_value = optimize_acqf(\n",
    "            ei,\n",
    "            bounds=torch.stack([tr_lb, tr_ub]),\n",
    "            q=batch_size,\n",
    "            num_restarts=num_restarts,\n",
    "            raw_samples=raw_samples,\n",
    "        )\n",
    "\n",
    "    return X_next\n",
    "\n",
    "\n",
    "# ## Optimization loop\n",
    "# This simple loop runs one instance of TuRBO-1 with Thompson sampling until convergence.\n",
    "# \n",
    "# TuRBO-1 is a local optimizer that can be used for a fixed evaluation budget in a multi-start fashion.  Once TuRBO converges, `state[\"restart_triggered\"]` will be set to true and the run should be aborted.  If you want to run more evaluations with TuRBO, you simply generate a new set of initial points and then keep generating batches until convergence or when the evaluation budget has been exceeded.  It's important to note that evaluations from previous instances are discarded when TuRBO restarts.\n",
    "# \n",
    "# NOTE: We use a `SingleTaskGP` with a noise constraint to keep the noise from getting too large as the problem is noise-free. \n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "X_turbo = get_initial_points(dim, n_init)\n",
    "Y_turbo = torch.tensor(\n",
    "    [eval_objective(x) for x in X_turbo], dtype=dtype, device=device\n",
    ").unsqueeze(-1)\n",
    "\n",
    "state = TurboState(dim, batch_size=batch_size, best_value=max(Y_turbo).item())\n",
    "\n",
    "NUM_RESTARTS = 10 #if not SMOKE_TEST else 2\n",
    "RAW_SAMPLES = 512 #if not SMOKE_TEST else 4\n",
    "N_CANDIDATES = 5000 #min(5000, max(2000, 200 * dim)) if not SMOKE_TEST else 4\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "while not state.restart_triggered:  # Run until TuRBO converges\n",
    "    # Fit a GP model\n",
    "    train_Y = (Y_turbo - Y_turbo.mean()) / Y_turbo.std()\n",
    "    likelihood = GaussianLikelihood(noise_constraint=Interval(1e-8, 1e-3))\n",
    "    covar_module = ScaleKernel(  # Use the same lengthscale prior as in the TuRBO paper\n",
    "        MaternKernel(\n",
    "            nu=2.5, ard_num_dims=dim, lengthscale_constraint=Interval(0.005, 4.0)\n",
    "        )\n",
    "    )\n",
    "    gp_model = SingleTaskGP(\n",
    "        X_turbo, train_Y, covar_module=covar_module, likelihood=likelihood\n",
    "    )\n",
    "    mll = ExactMarginalLogLikelihood(gp_model.likelihood, gp_model)\n",
    "\n",
    "    # Do the fitting and acquisition function optimization inside the Cholesky context\n",
    "    with gpytorch.settings.max_cholesky_size(max_cholesky_size):\n",
    "        # Fit the gp_model\n",
    "        fit_gpytorch_mll(mll)\n",
    "\n",
    "        # Create a batch\n",
    "        X_next = generate_batch(\n",
    "            state=state,\n",
    "            model=gp_model,\n",
    "            X=X_turbo,\n",
    "            Y=train_Y,\n",
    "            batch_size=batch_size,\n",
    "            n_candidates=N_CANDIDATES,\n",
    "            num_restarts=NUM_RESTARTS,\n",
    "            raw_samples=RAW_SAMPLES,\n",
    "            acqf=\"ts\",\n",
    "        )\n",
    "\n",
    "    Y_next = torch.tensor(\n",
    "        [eval_objective(x) for x in X_next], dtype=dtype, device=device\n",
    "    ).unsqueeze(-1)\n",
    "\n",
    "    # Update state\n",
    "    state = update_state(state=state, Y_next=Y_next)\n",
    "\n",
    "    # Append data\n",
    "    X_turbo = torch.cat((X_turbo, X_next), dim=0)\n",
    "    Y_turbo = torch.cat((Y_turbo, Y_next), dim=0)\n",
    "\n",
    "    # Print current status\n",
    "    print(\n",
    "        f\"{len(X_turbo)}) Best value: {state.best_value:.2e}, TR length: {state.length:.2e}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eebe1b-8b00-494f-98af-35129d60a816",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_min_params, loss, loss_min_list = optimize_pso(model, observed_rays, uncompensated_parameters_selected, iterations=2000, num_candidates=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1ecd44-20af-4a4d-8640-37dc46969f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_min_params, loss, loss_min_list = optimize_ea(model, observed_rays, uncompensated_parameters_selected, iterations=1000, num_candidates=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f3671d-08ec-44a9-98c4-d2996f0b389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_min_params, loss, loss_min_list = optimize_brute(model, observed_rays, uncompensated_parameters_selected, iterations=1000, num_candidates=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f650064-9e24-4a1b-ac4a-7a60d4d0dcc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_min_params, loss, loss_min_list = optimize_smart_walker(model, observed_rays, uncompensated_parameters_selected, iterations = 1000, num_candidates=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7484665b-0022-439e-850d-39a61aba177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = plot_param_tensors(loss_min_params[[1,2,4,8]], uncompensated_parameters_selected[[1,2,4,8]], engine = engine, ray_parameter_container=model.input_parameter_container, compensated_parameters=compensated_parameters_selected[[1,2,4,8]])\n",
    "\n",
    "method_dict = {\"smart walker\": (optimize_smart_walker, 1000), \"brute\": (optimize_brute, 1000), \"pso\": (optimize_pso, 1000), \"ea\": (optimize_ea, 1000), \"evo\": (optimize_evotorch, 1000)}\n",
    "method_evaluation_dict = {}\n",
    "method_dict = {\"ea\": (optimize_ea, 1000), \"evo\": (optimize_evotorch, 1000)}\n",
    "\n",
    "for key, entry in tqdm(method_dict.items(), desc=\"Evaluating methods\"):\n",
    "    loss_best, mean_progress, std_progress, loss_min_params_tens, offset_rmse = evaluate_evaluation_method(entry[0], model, repetitions=2, num_candidates=entry[1], iterations=10)\n",
    "    method_evaluation_dict[key]= (loss_best, mean_progress, std_progress, offset_rmse)\n",
    "\n",
    "def run_optimizer(optimizer, model, observed_rays, uncompensated_parameters, iterations, num_candidates):\n",
    "    return optimizer(model, observed_rays, uncompensated_parameters, iterations, num_candidates)\n",
    "\n",
    "repetitions= 2\n",
    "iterations = 20\n",
    "for key, (optimizer, num_candidates) in method_dict.items():\n",
    "    t0 = benchmark.Timer(\n",
    "        stmt='run_optimizer(optimizer, model, observed_rays, uncompensated_parameters_selected, iterations=iterations, num_candidates=num_candidates)',\n",
    "        setup='from __main__ import run_optimizer',\n",
    "        globals={'optimizer': optimizer, 'model': model, 'observed_rays': observed_rays, 'iterations': iterations, 'uncompensated_parameters_selected': uncompensated_parameters_selected, 'num_candidates': num_candidates},\n",
    "        num_threads=1,\n",
    "        label='optimize '+key,\n",
    "        sub_label=None)\n",
    "    timing_results = t0.blocked_autorange()\n",
    "    timings = torch.tensor(timing_results.times)\n",
    "    execution_time = torch.min(timings).item()\n",
    "    method_evaluation_dict[key] += (execution_time,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8916a065-2c8f-4f19-ad9e-ba5783df0286",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimizer_iterations(method_evaluation_dict, outputs_dir='../../outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4e4a4b-2460-44ab-82fa-da1a5bb75243",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = benchmark.Timer(\n",
    "    stmt='optimize_smart_walker(model, observed_rays, uncompensated_parameters_selected)',\n",
    "    setup='from __main__ import optimize_smart_walker',\n",
    "    globals={'model': model, 'observed_rays': observed_rays, 'uncompensated_parameters_selected': uncompensated_parameters_selected},\n",
    "    num_threads=1,\n",
    "    label='optimize smart walker',\n",
    "    sub_label='optimize smart walker')\n",
    "print(t0.timeit(repetitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4765ab94-0f73-4c31-b52d-b11ae0bfe002",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_observed_rays = observed_rays.squeeze()\n",
    "plot_min_param_rays = model(loss_min_params)\n",
    "fig, ax = plt.subplots(1, plot_observed_rays.shape[0], sharex=True, sharey=True, figsize=(32, 9))\n",
    "with torch.no_grad():\n",
    "    for i in range(plot_observed_rays.shape[0]):\n",
    "        ax[i].plot(model(loss_min_params)[i].cpu())\n",
    "        ax[i].plot(observed_rays.squeeze()[i].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b24b186-9011-4d51-a703-e160802abebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_tensors(loss_min_params[:,0], uncompensated_parameters_selected[:,0], engine = engine, ray_parameter_container=model.input_parameter_container, compensated_parameters=compensated_parameters_selected[:,0], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca2dfaf-7a6b-4d4a-a66c-1b0c9f4e2330",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = fancy_plot_param_tensors(loss_min_params[:], uncompensated_parameters_selected[:].squeeze(), engine = engine, ray_parameter_container=model.input_parameter_container, compensated_parameters=compensated_parameters_selected[:].squeeze())\n",
    "import plotly.io as pio\n",
    "\n",
    "# Save the figure to an HTML file\n",
    "pio.write_html(fig, 'figure.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060356b2-e008-4493-881b-75efef1252dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray_tools.base.transform import Histogram, RayTransformConcat, XYHistogram\n",
    "\n",
    "transforms = [\n",
    "        XYHistogram(50, (-10., 10.), (-3., 3.))\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, sharey=True, squeeze=False)\n",
    "\n",
    "x_simulation_hist_list = []\n",
    "y_simulation_hist_list = []\n",
    "for i in trange(2):\n",
    "    out = engine.run(tensor_list_to_param_container_list(loss_min_params), XYHistogram(50, (-10., 10.), (-3., 3.)))\n",
    "    out_simulation = out[-1]['ray_output']['ImagePlane']['0.0']\n",
    "    x_simulation_hist, _ = torch.histogram(out_simulation.x_loc,bins=50, range=[-10, 10])\n",
    "    y_simulation_hist, _ = torch.histogram(out_simulation.y_loc,bins=50, range=[-3, 3])\n",
    "    x_simulation_hist_list.append(x_simulation_hist / 22594.)\n",
    "    y_simulation_hist_list.append(y_simulation_hist / 22594.)\n",
    "    \n",
    "    ax[0, 0].plot(torch.linspace(-10, 10, 50), x_simulation_hist / 22594.)\n",
    "    ax[0, 1].plot(torch.linspace(-3, 3, 50), y_simulation_hist / 22594.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2835c97-20fc-4df7-9f82-e8a0460905bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_min_params.shape\n",
    "t = tensor_list_to_param_container_list(loss_min_params)\n",
    "out = [engine.run(tensor_list_to_param_container_list(loss_min_params), XYHistogram(50, (-10., 10.), (-3., 3.))) for i in trange(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dee67c-a53d-48d2-847e-a2dc43d1673a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be920a4b-df8d-4c6a-afd3-ee2860c7ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_min_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f93da-f748-4d9a-8cc9-7d83236cd629",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_list = []\n",
    "for i in out:\n",
    "    hist_list.append(torch.vstack([j['ray_output']['ImagePlane']['histogram'].reshape(-1) / 22594. for j in i]))\n",
    "#        plt.plot(j['ray_output']['ImagePlane']['histogram'].reshape(-1) / 22594.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97df58e-a7a2-4e45-a925-63b0bd0d6d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_list_tensor = torch.stack(hist_list)\n",
    "#print(hist_list_tensor.shape)\n",
    "print(\"var\",hist_list_tensor.var(dim=0, correction=0).mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4318fe85-2d93-4312-ac5a-b089c8f9c707",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf51d90-a077-4aea-98dd-d88b38fc077f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#bal_memory_dataset = BalancedMemoryDataset(dataset=dataset, load_len=load_len, min_n_rays=1)\n",
    "#memory_dataset = MemoryDataset(dataset=dataset, load_len=load_len)\n",
    "#datamodule = DefaultDataModule(dataset=bal_memory_dataset, num_workers=4)\n",
    "#datamodule.prepare_data()\n",
    "#datamodule.setup(stage=\"test\")\n",
    "#test_dl = datamodule.test_dataloader()\n",
    "\n",
    "#unbal_datamodule = DefaultDataModule(dataset=memory_dataset, num_workers=4)\n",
    "#unbal_datamodule.prepare_data()\n",
    "#unbal_datamodule.setup(stage=\"test\")\n",
    "\n",
    "#unbal_test_dl = unbal_datamodule.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9900b14-4283-4da0-9bc9-aa4db0406f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1987c7f4-09f8-4e1c-a4cd-b90f0ab74ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_statistics(mses_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98253db-b210-4ab2-983f-40c050995119",
   "metadata": {},
   "source": [
    "## Maximum distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b4e764-e7da-4764-9e8c-a66a8472e46f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "value_list = []\n",
    "params_list = []\n",
    "\n",
    "for i in tqdm(unbal_test_dl):\n",
    "    biggest = i[1].flatten(start_dim=1)\n",
    "    biggest, _ = i[1].flatten(start_dim=1).max(dim=1)\n",
    "    mask = biggest > 0.8\n",
    "    value_list.append(biggest[mask])\n",
    "    params_list.append(i[0][mask])\n",
    "value_tensor = torch.cat(value_list)\n",
    "params_tensor = torch.cat(params_list)\n",
    "\n",
    "torch.save(value_tensor, 'outputs/values.pt')\n",
    "torch.save(params_tensor, 'outputs/params.pt')\n",
    "plt.hist(value_tensor)\n",
    "plt.savefig('outputs/max_dist_hist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58de822f-7b2f-4b1b-8ce3-5075f5de708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_tensor = torch.load('outputs/values.pt')\n",
    "params_tensor = torch.load('outputs/params.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdc65fd-e313-4628-9287-388b7e95c562",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(test_dl):\n",
    "    #print(len(i[0][:10]))\n",
    "    t = tensor_list_to_param_container_list(i[0][:10])\n",
    "    print(len(t))\n",
    "    out = [engine.run(t, XYHistogram(50, (-10., 10.), (-3., 3.))) for i in trange(2)]\n",
    "    break\n",
    "#['ray_output']['ImagePlane']['histogram']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b620b358-ba48-49a2-ad0f-89e4a3dca9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out[0][0]['ray_output']['ImagePlane']['histogram'])\n",
    "len(out), len(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cb59fd-09a9-4d71-9f60-a35e971980d5",
   "metadata": {},
   "source": [
    "# Special sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a67b85-035a-4433-b00f-80af733a4479",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"outputs/special_sample_168_selected.pkl\", \"rb\") as f:\n",
    "    special_sample = pickle.load(f, fix_imports=True, encoding='ASCII', errors='strict', buffers=None)\n",
    "observed_params = special_sample.uncompensated_parameters\n",
    "\n",
    "for param_container in observed_params:\n",
    "    for label in ['ImagePlane.translationXerror', 'ImagePlane.translationYerror', 'ImagePlane.translationZerror']:\n",
    "        if label in list(param_container.keys()):\n",
    "            del param_container[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f574994c-891f-4241-8ebc-69923812263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(observed_params)\n",
    "Plot.plot_engines_comparison(engine, surrogate_engine, observed_params[:5], MultiLayer([0.]), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aedafb7-548b-47ee-bdb1-a33c346eda44",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncompensated_parameters = [elem.clone() for elem in special_sample.uncompensated_parameters]\n",
    "for elem in uncompensated_parameters:\n",
    "    elem.perturb(special_sample.target_params)\n",
    "uncompensated_parameters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903329a4-b9af-4627-a406-8d93a8b058f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot.plot_engines_comparison(engine, surrogate_engine, uncompensated_parameters, MultiLayer([0.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f251daa-c4f3-4461-af8d-37f6522f99c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_comparison, x_simulation_hist, y_simulation_hist = mse_engines_comparison(engine, surrogate_engine, uncompensated_parameters[:5], MultiLayer([0.]))\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "for hist in x_simulation_hist:\n",
    "    ax[0].plot(hist, alpha=0.3)\n",
    "for hist in y_simulation_hist:\n",
    "    ax[1].plot(hist, alpha=0.3)\n",
    "ax[2].hist(mse_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a5863a-6470-42a4-9f13-3f7b5abe0509",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_loc_list = []\n",
    "good_param_list = []\n",
    "batch_size = 5000\n",
    "for i in trange(15000//batch_size):\n",
    "    param_container = [tensor_to_param_container(torch.rand((34,))) for _ in range(batch_size)]\n",
    "    surrogate_out = surrogate_engine.run(param_container, MultiLayer([0.]))\n",
    "    for j in range(len(param_container)):\n",
    "        output = surrogate_out[j]['ray_output']['ImagePlane']['xy_hist']\n",
    "        if output.x_loc.sum() > 0.5:\n",
    "            x_loc_list.append(output.x_loc.sum())\n",
    "            good_param_list.append(param_container[j])\n",
    "\n",
    "observed_containers_tensor = torch.vstack([surrogate_engine.select({\"1e5/params\":param_container})[0] for param_container in observed_params])\n",
    "good_containers_tensor = torch.vstack([surrogate_engine.select({\"1e5/params\":param_container})[0] for param_container in good_param_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a886aeb-f0b4-4971-84e8-4b2b3662ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "for i in range(good_containers_tensor.shape[0]):\n",
    "    plt.plot(good_containers_tensor[i], c = 'blue', alpha=0.1)\n",
    "for i in range(observed_containers_tensor.shape[0]):\n",
    "    plt.plot(observed_containers_tensor[i], alpha=0.8)\n",
    "plt.legend([\"special\", \"good\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418cb63a-b773-4aab-854b-dec4755ccc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask = value_tensor > 0.44\n",
    "out = ((params_tensor - observed_containers_tensor[0].unsqueeze(0))**2)/2.\n",
    "out = out.mean(dim=1)\n",
    "out_sorted, indices = torch.sort(out)\n",
    "#part_indices = indices[:5]\n",
    "#print(part_indices.shape)\n",
    "#min_arg = out.argmin()\n",
    "#plt.hist(out.mean(dim=1))\n",
    "#plt.plot(params_tensor[min_arg])\n",
    "#plt.plot(observed_containers_tensor[0])\n",
    "for i in indices[:1]:\n",
    "    plt.plot(params_tensor[i])\n",
    "Plot.plot_engines_comparison(engine, surrogate_engine, [tensor_to_param_container(params_tensor[min_arg]) for min_arg in indices[:1]], MultiLayer([0.]), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e4311-ab69-4692-a340-5f0cb3342039",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = {\"ImagePlane\": transform for transform in cfg_transforms}\n",
    "out_engine = engine.run(observed_params, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636cd240-dc8b-48bf-937c-7c39caa881fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_simulations = surrogate_engine.model.standardizer(torch.vstack([element['ray_output']['ImagePlane']['histogram'].flatten(start_dim=0) for element in out_engine]))\n",
    "a = ((standardized_simulations - out_model)**2).mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7500019-8d32-4055-b085-3311b7bdeb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a933e278-11bf-4216-bdb2-e6643ed5d668",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Good params vs. bad params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7513437-7f9c-44b0-8793-e9549a488ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list = []\n",
    "num_rays_list = []\n",
    "for i in tqdm(unbal_test_dl):\n",
    "    params_list.append(i[0])\n",
    "    num_rays_list.append(i[1])\n",
    "params_tensor = torch.vstack(params_list)\n",
    "num_rays_tensor= torch.vstack(num_rays_list)\n",
    "plt.hist(torch.tensor(num_rays_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0f571f-4de1-43c0-b71e-df26e91d5d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest = torch.tensor(num_rays_list).argmax()\n",
    "test_parameters = memory_dataset[biggest][0]\n",
    "param_container_list = [tensor_to_param_container(test_parameters)]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, sharey=True, squeeze=False)\n",
    "\n",
    "x_simulation_hist_list = []\n",
    "y_simulation_hist_list = []\n",
    "for i in trange(20):\n",
    "    out = engine.run(param_container_list, MultiLayer([0.]))\n",
    "    out_simulation = out[-1]['ray_output']['ImagePlane']['0.0']\n",
    "    x_simulation_hist, _ = torch.histogram(out_simulation.x_loc,bins=50, range=[-10, 10])\n",
    "    y_simulation_hist, _ = torch.histogram(out_simulation.y_loc,bins=50, range=[-3, 3])\n",
    "    x_simulation_hist_list.append(x_simulation_hist / 22594.)\n",
    "    y_simulation_hist_list.append(y_simulation_hist / 22594.)\n",
    "    \n",
    "    ax[0, 0].plot(torch.linspace(-10, 10, 50), x_simulation_hist / 22594.)\n",
    "    ax[0, 1].plot(torch.linspace(-3, 3, 50), y_simulation_hist / 22594.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aed3db-467f-48ef-8886-f39faab1cc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_simulation_hist_tens = torch.vstack(x_simulation_hist_list)\n",
    "y_simulation_hist_tens = torch.vstack(y_simulation_hist_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f043fc52-8c01-487f-a2f8-cef90a8be1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_simulation_hist_tens.var(dim=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc62cae5-f272-41db-b531-ca271b79475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_simulation_hist_tens.mean(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0724b0a1-ceae-44fa-b059-099007dfda3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_tensor = hist_dataset.data_dict['histogram/ImagePlane']#['parameters']\n",
    "num_rays_tensor = hist_dataset.data_dict['n_rays/ImagePlane']\n",
    "print(params_tensor.shape, num_rays_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8463884f-efac-4924-acb9-bcf050cc3494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "mask = num_rays_tensor >= 5.# 100.\n",
    "data_tensor = params_tensor[mask]\n",
    "print(data_tensor.shape)\n",
    "data_tensor = data_tensor[:,:]\n",
    "class_tensor = num_rays_tensor[mask]\n",
    "\n",
    "data_np = data_tensor.numpy()\n",
    "class_np = class_tensor.numpy().flatten()\n",
    "print(data_np.shape)\n",
    "umap_model = umap.UMAP(n_neighbors=20, min_dist=0.1, n_components=2)\n",
    "umap_embedding = umap_model.fit_transform(data_np)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(umap_embedding[:, 0], umap_embedding[:, 1], c=class_np, cmap='Spectral', s=5)\n",
    "plt.colorbar(scatter)\n",
    "plt.title('UMAP projection of the dataset')\n",
    "plt.xlabel('UMAP 1')\n",
    "plt.ylabel('UMAP 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c90389-1e6b-47f6-9bc2-46cd1a0c8c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = torch.vstack([model(cp) for cp in compensated_parameters_list]).cpu()\n",
    "ct = torch.cat([torch.ones((len(compensated_parameters_list[i])*dt.shape[1],)).float() * i for i in range(len(compensated_parameters_list))]).cpu()\n",
    "#ct = torch.where(ct == 0, 0., 1.)\n",
    "#dt = dt.flatten(start_dim=0, end_dim=-2)\n",
    "print(dt.shape, ct.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d06887-4c1b-4d2e-bbdd-780666e80b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "mask = num_rays_tensor >= 1000.# 100.\n",
    "print(observed_rays_real.shape)\n",
    "observed_real_tensor = observed_rays_real.flatten(start_dim=0, end_dim=-2).cpu()\n",
    "observed_sim_tensor = hist_dataset.data_dict['histogram/ImagePlane'][mask] #observed_rays.flatten(start_dim=0, end_dim=-2).cpu()#\n",
    "\n",
    "observed_real_label = torch.ones((observed_real_tensor.shape[0],)).float()\n",
    "observed_sim_label = torch.ones((observed_sim_tensor.shape[0],)).float() * 2.\n",
    "\n",
    "params_tensor\n",
    "#\n",
    "data_tensor = torch.cat((observed_real_tensor, observed_sim_tensor))\n",
    "class_tensor = torch.cat((observed_real_label, observed_sim_label))\n",
    "\n",
    "data_np = data_tensor.numpy()\n",
    "class_np = class_tensor.numpy().flatten()\n",
    "print(data_np.shape)\n",
    "umap_model = umap.UMAP(n_neighbors=300, min_dist=0.1, n_components=2)\n",
    "umap_embedding = umap_model.fit_transform(data_np)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(umap_embedding[:, 0], umap_embedding[:, 1], c=class_np, cmap='Spectral', s=5)\n",
    "plt.colorbar(scatter)\n",
    "plt.title('UMAP projection of the dataset')\n",
    "plt.xlabel('UMAP 1')\n",
    "plt.ylabel('UMAP 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d9f5cc-8a37-45c4-9453-816dd4f2aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_real_tensor = observed_rays_real.flatten(start_dim=0, end_dim=-2).cpu()\n",
    "observed_sim_tensor = hist_dataset.data_dict['histogram/ImagePlane'][mask] #observed_rays.flatten(start_dim=0, end_dim=-2).cpu()#\n",
    "\n",
    "observed_real_tensor.shape, observed_sim_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f774e8-9ea0-49c3-850a-49bd9bcefaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_rays_real.flatten(start_dim=1, end_dim=-1).cpu().shape #.flatten(start_dim=0, end_dim=-2).cpu().shape\n",
    "dt.flatten(start_dim=1, end_dim=-1).cpu().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04008679-7920-4c8d-842b-77e40c2fed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_umap(umap_embedding, class_np):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(umap_embedding[:, 0], umap_embedding[:, 1], c=class_np, cmap='bwr', s=5)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title('UMAP projection of the dataset')\n",
    "    plt.xlabel('UMAP 1')\n",
    "    plt.ylabel('UMAP 2')\n",
    "    plt.show()\n",
    "\n",
    "def calculate_umap(observed_real_tensor, observed_sim_tensor, n_neighbors=300, min_dist=0.1):\n",
    "    observed_real_label = torch.ones((observed_real_tensor.shape[0],)).float()\n",
    "    observed_sim_label = torch.ones((observed_sim_tensor.shape[0],)).float() * 3.\n",
    "    \n",
    "    \n",
    "    data_tensor = torch.cat((observed_real_tensor, observed_sim_tensor))\n",
    "    class_tensor = torch.cat((observed_real_label, observed_sim_label))\n",
    "    print(data_tensor.shape, class_tensor.shape)\n",
    "    \n",
    "    data_np = data_tensor.numpy()\n",
    "    class_np = class_tensor.numpy().flatten()\n",
    "    \n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=2)\n",
    "    umap_embedding = umap_model.fit_transform(data_np)\n",
    "    return umap_embedding, class_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466ab306-0acb-46b2-8811-87f7a5fd4e43",
   "metadata": {},
   "source": [
    "## Generated problems z-included UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9472b9-ce97-4ba7-acca-ff45f9d21292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "observed_real_tensor = observed_rays_real.flatten(start_dim=1, end_dim=-1).cpu()\n",
    "observed_sim_tensor = dt.flatten(start_dim=1, end_dim=-1).cpu()\n",
    "\n",
    "umap_embedding, class_np = calculate_umap(observed_real_tensor, observed_sim_tensor, n_neighbors=2, min_dist=0.01)\n",
    "plot_umap(umap_embedding, class_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c53cf-1880-4699-8648-2741625cec0a",
   "metadata": {},
   "source": [
    "## Generated problems all beamlines z-included UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10d9361-1ada-4919-8af7-a15f56002c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_sim_tensor = torch.stack([model(cp)[:14] for cp in compensated_parameters_list]).cpu().flatten(start_dim=1)\n",
    "observed_real_tensor = observed_rays_real[:14].unsqueeze(0).cpu().flatten(start_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289de7ed-7a55-4185-912f-c66c7dd102db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#observed_sim_tensor.shape, observed_real_tensor.shape\n",
    "umap_embedding, class_np = calculate_umap(observed_real_tensor, observed_sim_tensor[:5000], n_neighbors=10, min_dist=0.9)\n",
    "plot_umap(umap_embedding, class_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0bf724-e188-4443-b1c9-bcc265cb445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne_model = TSNE(n_components=2, perplexity=30, learning_rate=200, max_iter=1000)\n",
    "tsne_embedding = tsne_model.fit_transform(data_np)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(tsne_embedding[:, 0], tsne_embedding[:, 1], c=class_np, cmap='Spectral', s=5)\n",
    "plt.colorbar(scatter)\n",
    "plt.title('t-SNE projection of the dataset')\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a61f44c-0ba3-4eec-97c2-31fdeac4cb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from ray_nn.nn.xy_hist_data_models import MetrixXYHistSurrogate, StandardizeXYHist\n",
    "from ray_tools.simulation.torch_datasets import MemoryDataset, HistDataset\n",
    "from datasets.metrix_simulation.config_ray_emergency_surrogate import PARAM_CONTAINER_FUNC as params\n",
    "from torch.utils.data import DataLoader\n",
    "from ray_nn.data.transform import Select\n",
    "\n",
    "model_path = \"../../outputs/xy_hist/ee8cvj82/checkpoints/epoch=36-step=35212012.ckpt\"\n",
    "model = MetrixXYHistSurrogate.load_from_checkpoint(model_path)\n",
    "model.to(torch.device('cpu'))\n",
    "model.compile()\n",
    "model.eval()\n",
    "\n",
    "load_len: int | None = 1000\n",
    "h5_files = list(glob.iglob('datasets/metrix_simulation/ray_emergency_surrogate_50+50+z/histogram_*.h5'))\n",
    "sub_groups = ['parameters', 'histogram/ImagePlane', 'n_rays/ImagePlane']\n",
    "transforms=[lambda x: x[1:].float(), lambda x: standardizer(x.flatten().float()), lambda x: x.int()]\n",
    "dataset = HistDataset(h5_files, sub_groups, transforms, normalize_sub_groups=['parameters'], load_max=load_len)\n",
    "\n",
    "\n",
    "memory_dataset = MemoryDataset(dataset=dataset, load_len=load_len)\n",
    "\n",
    "train_dataloader = DataLoader(memory_dataset, batch_size=2048, shuffle=False, num_workers=0)\n",
    "\n",
    "errors_list = []\n",
    "with torch.no_grad():\n",
    "    for par_input, label, _ in tqdm(train_dataloader):\n",
    "        out = model(par_input)\n",
    "        label = label.flatten(start_dim=1)\n",
    "        b = ((label - out)**2).mean(dim=1)\n",
    "        errors_list.append(b)\n",
    "errors_tensor = torch.cat(errors_list)\n",
    "\n",
    "plt.hist(errors_tensor)\n",
    "plt.savefig('outputs/dataset_errors_hist.png')\n",
    "torch.save(errors_tensor, 'outputs/dataset_errors.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26899f5-568d-4447-af06-fc8786606588",
   "metadata": {},
   "source": [
    "# Import real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2faae3b-c24d-42dc-89fd-7043dd4eaf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sub_projects.ray_optimization.real_data import import_data #, \n",
    "#[-15., -10., -5., 0., 5., 10., 15., 20., 25., 30.]\n",
    "def import_real_hist_data(parameter_container, device, path = '../../datasets/metrix_real_data/2021_march_complete', import_set = ['M10', 'M18', 'M22', 'M23', 'M24', 'M25', 'M27', 'M28', 'M29', 'M30', 'M32', 'M33', 'M36',\n",
    "                             'M37', 'M40', 'M41', 'M42', 'M43', 'M44'], z_layers=[-15., -10., -5., 0., 5., 10., 15., 20., 25., 30.], check_value_lims=False, z_array_label='ImagePlane.translationZerror'):\n",
    "    imported_data = import_data(\n",
    "                path,\n",
    "                import_set,\n",
    "                z_layers,\n",
    "                parameter_container,\n",
    "                check_value_lims=check_value_lims,\n",
    "            )\n",
    "    xy_hist = XYHistogram(50, (-10., 10.), (-3., 3.))\n",
    "    z_array_min, z_array_max = model.input_parameter_container[z_array_label].value_lims\n",
    "    normalized_z_array = torch.tensor((z_layers - z_array_min) / (z_array_max - z_array_min), device=device).float()\n",
    "\n",
    "    # observed_rays_point_cloud\n",
    "    real_data_point_cloud_list = []\n",
    "    for i in range(len(imported_data)):\n",
    "        real_data_point_cloud_list.append(imported_data[i])\n",
    "    observed_rays_point_cloud = [ray_dict_to_tensor(entry, 'ImagePlane') for entry in real_data_point_cloud_list]\n",
    "\n",
    "    real_data_list = []\n",
    "    for i in range(len(imported_data)):\n",
    "        real_data_list.append(xy_hist(imported_data[i]['ray_output']['ImagePlane']))\n",
    "    z_layer_real_data_tensor_list = []\n",
    "    for z in z_layers:\n",
    "        z_layer_real_data_tensor_list.append(torch.stack([real_data_list[i][z]['histogram'] for i in range(len(real_data_list))]))\n",
    "    real_data_tensor = torch.stack(z_layer_real_data_tensor_list, dim=1)\n",
    "\n",
    "    real_data_tensor_normalized = real_data_tensor / 4000. #surrogate_engine.model.standardizer(real_data_tensor)\n",
    "    observed_rays = real_data_tensor_normalized.flatten(start_dim=-2).unsqueeze(2).float().to(device)\n",
    "\n",
    "    uncompensated_parameters_list = []\n",
    "    for i in range(len(imported_data)):\n",
    "        uncompensated_entry = torch.tensor([value.get_value() for value in Plot.normalize_parameters(imported_data[i]['param_container_dict'], parameter_container).values()])\n",
    "        uncompensated_parameters_list.append(uncompensated_entry)\n",
    "    uncompensated_parameters = torch.stack(uncompensated_parameters_list)\n",
    "    uncompensated_parameters = uncompensated_parameters.unsqueeze(1).float().to(device)\n",
    "    uncompensated_parameters = uncompensated_parameters.repeat_interleave(len(z_layers), dim=1).unsqueeze(2)\n",
    "    uncompensated_parameters[:, :, 0, -1] = normalized_z_array\n",
    "    \n",
    "    #print(uncompensated_parameters)\n",
    "    \n",
    "    return observed_rays, uncompensated_parameters, observed_rays_point_cloud\n",
    "    \n",
    "#observed_rays_real, uncompensated_parameters_real, observed_rays_point_cloud = import_real_hist_data(model.input_parameter_container, model.device, path = '../../datasets/metrix_real_data/2021_march_complete', import_set = ['M03'], z_layers=[0., 5., 10.], check_value_lims=False, z_array_label='ImagePlane.translationZerror')\n",
    "#plt.clf()\n",
    "#plt.plot(observed_rays_real[0][0][0].cpu())\n",
    "#plt.show()   \n",
    "observed_rays_real, uncompensated_parameters_real, observed_rays_point_cloud = import_real_hist_data(model.input_parameter_container, device=model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedc199c-c51e-43e3-b5b1-aa1199f2014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1000042\n",
    "loss_min_params, loss, loss_min_list = optimize_evotorch_ga(model, observed_rays_real, uncompensated_parameters_real, iterations=1000, num_candidates=500, mutation_scale=0.2, sbx_crossover_rate=0.8, tournament_size=3, seed=seed)\n",
    "compensated_rays = model(loss_min_params.clone())\n",
    "fig = MetrixXYHistSurrogate.plot_data_3(compensated_rays[:3, 0].cpu(), observed_rays_real[:3, 0,0].cpu(), label_list=[\"Compensated\", \"Experiment\"])\n",
    "plt.savefig('../../outputs/real_world_optimized.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52926a0b-f723-496d-84e0-8b46db6112ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_min_params, loss, loss_min_list  = optimize_smart_walker(model, observed_rays_real, uncompensated_parameters_real, iterations=1000, num_candidates=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ec1683-db55-411e-94f5-c232488db214",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.set_default_device('cuda')\n",
    "print(loss_min_params.shape)\n",
    "pc = [tensor_to_param_container(loss_min_params[i][0].cpu(), ray_parameter_container=model.input_parameter_container) for i in range(loss_min_params.shape[0])]\n",
    "Plot.plot_engines_comparison(engine, surrogate_engine, pc[:8], MultiLayer([0.]), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09d0c91-c696-4849-8df7-b60fb401b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_engines_comparison_2(input_param_tensor, engine, model):\n",
    "    engine_outputs = simulate_param_tensor(input_param_tensor, engine, ray_parameter_container=model.input_parameter_container, exported_plane='ImagePlane')\n",
    "    std_backward = model.standardizer.destandardize\n",
    "    model_outputs = std_backward(model(input_param_tensor)).detach().cpu()\n",
    "\n",
    "    fig, ax = plt.subplots(len(engine_outputs),2 * model_outputs[0].shape[0], sharey=True, squeeze=False, figsize=(14,5))\n",
    "    for i in range(len(engine_outputs)):\n",
    "        for j in range(model_outputs[0].shape[0]):\n",
    "            model_hist = model_outputs[i, j]\n",
    "            out_simulation = engine_outputs[i][j]\n",
    "\n",
    "            intensity = (model.intensity_multiplier * input_param_tensor[i, j, -4] + model.intensity_addend).unsqueeze(-1).cpu()\n",
    "            #print(input_param_tensor[i, j, -4])\n",
    "            x_simulation_hist, _ = torch.histogram(out_simulation[:, 0],bins=50, range=[-10, 10])\n",
    "            line1, = ax[i, 2*j].plot(torch.linspace(-10, 10, 50).cpu(), x_simulation_hist.cpu()*intensity, label=\"simulation\")\n",
    "            line2, = ax[i, 2*j].plot(torch.linspace(-10, 10, 50).cpu(), model_hist[:50], label=\"surrogate\")\n",
    "            y_simulation_hist, _ = torch.histogram(out_simulation[:, 1],bins=50, range=[-3, 3]) \n",
    "            ax[i, 2*j+1].plot(torch.linspace(-3, 3, 50).cpu(), y_simulation_hist.cpu()*intensity)\n",
    "            ax[i, 2*j+1].plot(torch.linspace(-3, 3, 50).cpu(), model_hist[50:])\n",
    "            ax[0, 0].legend(handles=[line1, line2])\n",
    "        \n",
    "    return fig\n",
    "\n",
    "plot_engines_comparison_2(loss_min_params[:10, :1], engine, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f68eb85-70c4-483f-ae84-1e99a4ca8f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_min_params.shape, loss_min_params[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5c8615-1b8a-40b1-a4f4-70d77219990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = fancy_plot_param_tensors(loss_min_params[:], uncompensated_parameters_real[:].squeeze(2), engine = engine, ray_parameter_container=model.input_parameter_container, observed_rays_point_cloud=observed_rays_point_cloud)\n",
    "pio.write_html(fig, os.path.join('fancy.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc1be8-1bbe-4632-93ee-24c42769389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "print(observed_rays_real.shape)\n",
    "fig, ax = plt.subplots(5)\n",
    "for i in range(5):\n",
    "    for j in range(3):\n",
    "#ax[0].plot(model(loss_min_params[0])[0].cpu())\n",
    "       ax[i].plot(observed_rays_real[i][j][0].cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c4146a-eb16-47b7-8c1f-61368e0cbe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncompensated_parameters_real[:, :1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb2c876-88bd-41e1-a73c-088c9b82b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_tensors(loss_min_params[:,:1], uncompensated_parameters_real[:, :1].squeeze(1), engine=engine, ray_parameter_container=model.input_parameter_container, observed_rays_point_cloud=observed_rays_point_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f2f2b8-bac3-49ac-98eb-3d8fc157054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_tensors(loss_min_params[:], uncompensated_parameters_selected[:], compensated_parameters=compensated_parameters_selected[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccf28d6-559e-4c95-bc95-360d71a6eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_params = torch.vstack(10*[loss_min_params[1]])\n",
    "a = repeated_params.clone()\n",
    "b = repeated_params.clone()\n",
    "c = repeated_params.clone()\n",
    "for i, ten in enumerate(a):\n",
    "    ten[-1]=0.5+0.1*i\n",
    "    ten[-2]=0.5\n",
    "for i, ten in enumerate(b):\n",
    "    ten[-2]=0.5+0.1*i\n",
    "    ten[-1]=0.5\n",
    "for i, ten in enumerate(c):\n",
    "    ten[-1]=0.5\n",
    "    ten[-2]=0.5#+0.005*i\n",
    "plot_param_tensors(a[:3], b[:3], compensated_parameters=c[:3])\n",
    "#print(repeated_params)\n",
    "#plot_param_tensors(loss_min_params[:], uncompensated_parameters_selected[:], compensated_parameters=compensated_parameters_selected[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a87724d-4ca2-433d-9957-e4f8ae902631",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_min_params.shape, uncompensated_parameters_selected.shape, compensated_parameters_selected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0e49fe-5a57-4edf-a8f4-fab2f552d124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ef0d19-7c06-4a60-acc3-b5114c15cd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_comparison_plot = Plot.plot_param_comparison(\n",
    "    predicted_params=tensor_list_to_param_container_list(loss_min_params.squeeze().unsqueeze(0)),\n",
    "    epoch=42,\n",
    "    training_samples_count=len(observed_rays),\n",
    "    search_space=model.offset_space,\n",
    "    real_params=tensor_list_to_param_container_list(compensated_parameters_selected)[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b217c54c-b558-41f1-9e7a-66b78f1bdc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_comparison_plot = Plot.plot_param_comparison(\n",
    "    predicted_params=tensor_to_param_container(offsets_selected.squeeze()),\n",
    "    epoch=42,\n",
    "    training_samples_count=len(observed_rays),\n",
    "    search_space=RayOptimization.limited_search_space(model.offset_space, RandomGenerator(42), max_deviation=max_offset),\n",
    "    real_params=tensor_list_to_param_container_list(loss_min_params[0] - compensated_parameters_selected.squeeze())[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81738c2-10f3-4014-9c32-76f97bbe2fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_params = (loss_min_params - compensated_parameters_selected.squeeze())[0][0]\n",
    "real_params = offsets_selected[0].cpu()\n",
    "Plot.plot_normalized_param_comparison(\n",
    "    predicted_params=model.unscale_offset(predicted_params).cpu(),\n",
    "    labels= [key for key, value in model.input_parameter_container.items() if isinstance(value, MutableParameter)],\n",
    "    real_params=real_params,\n",
    ")\n",
    "plt.savefig('test.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff7217-c879-4ec9-84b7-3788d7a4fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_min_params.shape, uncompensated_parameters_selected.squeeze(-2).shape\n",
    "offsets_predicted = (loss_min_params-uncompensated_parameters_selected.squeeze(-2))[0][0]\n",
    "mutable_keys = [key for key, value in model.input_parameter_container.items() if isinstance(value, MutableParameter)]\n",
    "offsets_predicted_normalized = model.unscale_offset(offsets_predicted)\n",
    "offsets_predicted_normalized.min(), offsets_predicted_normalized.max()\n",
    "plot_param_comparison(\n",
    "    predicted_params=offsets_predicted_normalized.cpu(),\n",
    "    labels= mutable_keys,\n",
    "    real_params=offsets_selected[0].cpu()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bef591-1594-40af-bd3c-e862a9603f56",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a98980-c87c-4bd4-a775-e92dc0062c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10000\n",
    "x = torch.rand((batch_size, uncompensated_parameters.shape[-1]), requires_grad=True, device=model.device)\n",
    "optimize_algo = 'adam'\n",
    "if optimize_algo == 'adam':\n",
    "    optimizer = torch.optim.Adam([x], lr=0.01)  # You can adjust the learning rate as needed\n",
    "else:\n",
    "    optimizer = torch.optim.SGD([x], lr=0.01)  # Adjust the learning rate as needed\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "num_epochs = 1000  # Number of iterations\n",
    "uncompensated_parameters = uncompensated_parameters_selected\n",
    "#with torch.no_grad():\n",
    "observed_rays = model(compensated_parameters_selected, clone_output=True, grad=True)\n",
    "\n",
    "def function_to_minimize(x):\n",
    "    tensor_sum = x + uncompensated_parameters\n",
    "    compensated_rays = model(tensor_sum, clone_output=True, grad=True)\n",
    "    compensated_rays = compensated_rays.flatten(start_dim=2)\n",
    "    loss_orig = ((compensated_rays - observed_rays) ** 2).mean(0).mean(-1)\n",
    "    return loss_orig\n",
    "\n",
    "pbar = trange(num_epochs)\n",
    "for epoch in pbar:\n",
    "    optimizer.zero_grad()  # Clear the gradients from the previous step\n",
    "    \n",
    "    output = function_to_minimize(x.detach())  # Compute the function's output\n",
    "    loss = output.mean()  # Compute the mean loss for this batch\n",
    "    \n",
    "    loss.backward(retain_graph=True)  # Backpropagate to compute the gradients\n",
    "    optimizer.step()  # Update the parameters with SGD\n",
    "\n",
    "    # Optionally print the loss to monitor progress\n",
    "    if epoch % 100 == 0:\n",
    "        pbar.set_postfix({\"loss\": loss.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bd54f5-68db-446d-8eb4-819d6590b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.randn(100, 2)\n",
    "b= torch.randn(100, 2)\n",
    "loss = torch.nn.MSELoss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372aab00-3cb2-42c9-bf1e-6c23ad8070f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(a,b).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
