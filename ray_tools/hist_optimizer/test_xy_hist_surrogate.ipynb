{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab8113-e0e8-44e6-a38c-a7889f640d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import glob\n",
    "from tqdm.auto import tqdm, trange\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "import matplotlib.pyplot as plt\n",
    "from hist_optimizer import tensor_to_param_container, mse_engines_comparison, find_good_offset_problem, optimize_smart_walker, optimize_brute, optimize_pso, optimize_ea, evaluate_evaluation_method, plot_param_tensors, tensor_list_to_param_container_list, param_tensor_to_ray_outputs, compare_with_reference, fancy_plot_param_tensors\n",
    "\n",
    "from ray_tools.base.transform import MultiLayer\n",
    "from ray_tools.base.engine import RayEngine\n",
    "from ray_nn.data.lightning_data_module import DefaultDataModule\n",
    "from ray_nn.nn.xy_hist_data_models import MetrixXYHistSurrogate, StandardizeXYHist, HistSurrogateEngine, Model\n",
    "from ray_tools.base.backend import RayBackendDockerRAYUI\n",
    "from ray_tools.simulation.torch_datasets import BalancedMemoryDataset, MemoryDataset, RayDataset\n",
    "from ray_optim.plot import Plot\n",
    "from ray_tools.base.transform import Histogram, RayTransformConcat, XYHistogram\n",
    "from ray_nn.data.transform import Select\n",
    "from ray_tools.base.parameter import NumericalParameter, OutputParameter, NumericalOutputParameter, MutableParameter, RayParameterContainer\n",
    "from sub_projects.ray_optimization.real_data import import_data\n",
    "from sub_projects.ray_optimization.utils import ray_dict_to_tensor, ray_output_to_tensor\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe67c8a6-0a58-49d5-8c68-a6c129ca6eb2",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b643820-f6e9-49cb-97e2-6a96f940fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = RayEngine(rml_basefile='../../rml_src/METRIX_U41_G1_H1_318eV_PS_MLearn_1.15.rml',\n",
    "                                exported_planes=[\"ImagePlane\"],\n",
    "                                ray_backend=RayBackendDockerRAYUI(docker_image='ray-ui-service',\n",
    "                                                                  docker_container_name='ray-ui-service-test',\n",
    "                                                                  dockerfile_path='../../ray_docker/rayui',\n",
    "                                                                  ray_workdir='/dev/shm/ray-workdir',\n",
    "                                                                  verbose=False),\n",
    "                                num_workers=-1,\n",
    "                                as_generator=False)\n",
    "\n",
    "\n",
    "model_path = \"../../outputs/xy_hist/ft1rr9h0/checkpoints/epoch=68-step=65665644.ckpt\"\n",
    "surrogate_engine = HistSurrogateEngine(checkpoint_path=model_path)\n",
    "\n",
    "model = Model(path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a50f86d-fcb8-493a-afea-27db01664f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from ipywidgets import interact\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(X, Y, Z):\n",
    "    inp = torch.ones((1,model.mutable_parameter_count), device=model.device)*0.5\n",
    "    inp[:,-3] = X\n",
    "    inp[:,-2] = Y\n",
    "    inp[:,-4] = Z\n",
    "    ##out = model(inp)\n",
    "    out = surrogate_engine.run([tensor_to_param_container(inp[0], model.input_parameter_container)])\n",
    "    #out =  engine.run([tensor_to_param_container(inp[0], model.input_parameter_container)])\n",
    "    #out = [XYHistogram(50, (-10., 10.), (-3., 3.))(out[0]['ray_output']['ImagePlane'])['histogram'].flatten()]\n",
    "    out = [torch.cat((out[0]['ray_output']['ImagePlane']['xy_hist'].x_loc, out[0]['ray_output']['ImagePlane']['xy_hist'].y_loc))]\n",
    "    return out[0].detach().cpu().numpy()#A*x**2 + B*x + Z\n",
    "\n",
    "#fig = plt.figure()\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "histogram_lims = model.histogram_lims\n",
    "lineX, = ax[0].plot(np.linspace(histogram_lims[0][0], histogram_lims[0][1], num=50), f(X=.5, Y=.5, Z=.0)[:50])\n",
    "lineY, = ax[1].plot(np.linspace(histogram_lims[1][0], histogram_lims[1][1], num=50), f(X=.5, Y=.5, Z=.0)[50:])\n",
    "\n",
    "def update(X = .5, Y = .5, Z = .0):\n",
    "    new_f = f(X,Y,Z)\n",
    "    new_x = new_f[:50]\n",
    "    lineX.set_ydata(new_x)\n",
    "    ax[0].set_ylim(new_x.min(), new_x.max())\n",
    "    new_y = new_f[50:]\n",
    "    lineY.set_ydata(new_y)\n",
    "    ax[1].set_ylim(new_y.min(), new_y.max())\n",
    "    fig.canvas.draw_idle()\n",
    "    \n",
    "interact(update, X = (0,1,0.05), Y = (0,1,0.05), Z = (0,1,0.1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a6bb54-8d38-4597-9756-0aa567b8b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_list = []\n",
    "for i in range(20):\n",
    "    inp = torch.ones(model.mutable_parameter_count, device=model.device)*0.5\n",
    "    #inp[-3] = 0.5# + i*0.5\n",
    "    inp[-4] = 0. + i*0.05\n",
    "    inp_list.append(inp)\n",
    "\n",
    "#Plot.plot_engines_comparison(engine, surrogate_engine, [tensor_to_param_container(inp, model.input_parameter_container) for inp in inp_list], MultiLayer([0.]))\n",
    "inp = torch.stack(inp_list)\n",
    "plot_param_tensors( inp, inp, engine = engine, ray_parameter_container=model.input_parameter_container, compensated_parameters = inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435f5fe1-0fd6-4768-bfca-6a04886402bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(inp)\n",
    "plotted, _ = out.max(dim=-1)\n",
    "plt.plot(plotted.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19d0003-49c6-4b29-8ae4-3a0e3eadc779",
   "metadata": {},
   "source": [
    "# Look with model for new sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970756bd-471c-4fac-a271-ee04c7adaf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets_selected, uncompensated_parameters_selected, compensated_parameters_selected = find_good_offset_problem(model, fixed_parameters = [8, 14, 20, 21, 27, 28])\n",
    "\n",
    "with torch.no_grad():\n",
    "    observed_rays = model(compensated_parameters_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eca460-d26c-4d90-90e3-c73027e1af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets_list = []\n",
    "uncompensated_parameters_list = []\n",
    "compensated_parameters_list = []\n",
    "for i in range(50):\n",
    "    offsets_selected, uncompensated_parameters_selected, compensated_parameters_selected = find_good_offset_problem(model)\n",
    "    offsets_list.append(offsets_selected)\n",
    "    uncompensated_parameters_list.append(uncompensated_parameters_selected)\n",
    "    compensated_parameters_list.append(compensated_parameters_selected)\n",
    "\n",
    "offsets_list = torch.stack(offsets_list)\n",
    "print(offsets_selected.mean().item(), \"±\", offsets_selected.std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5067f041-c0cb-4e60-a89a-700ce5cdb6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data = uncompensated_parameters_selected[:, 0, 0, :]\n",
    "fig, ax = plt.subplots(2,1, figsize=(32,8))\n",
    "\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "# Plot each line with a different color from the cycle\n",
    "for i, y in enumerate(uncompensated_parameters_list):\n",
    "    ax[0].plot(y[:, 10, 0].T.cpu(), color=colors[i % len(colors)], alpha=0.2)\n",
    "    ax[0].set_title(\"Uncompensated parameters\")\n",
    "    ax[0].set_xlabel(\"Beamline parameter [#]\")\n",
    "    ax[0].set_xticks(torch.arange(37))\n",
    "    ax[1].plot(compensated_parameters_list[i][:, 10, 0].T.cpu(), color=colors[i % len(colors)], alpha=0.2)\n",
    "    ax[1].set_title(\"Compensated parameters\")\n",
    "    ax[1].set_xlabel(\"Beamline parameter [#]\")\n",
    "    ax[1].set_xticks(torch.arange(37))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9910d1a-ef38-46bd-8a4c-f383c7484a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, sharey=True)\n",
    "for entry in offsets_list:\n",
    "    ax[0].scatter(entry[0].detach().cpu(), y=torch.arange(37).detach().cpu())\n",
    "    ax[0].set_ylabel(\"Beamline parameter [#]\")\n",
    "    ax[0].set_xlabel(\"Chosen offset\")\n",
    "    ax[0].set_title(\"Simulated real beamline\")\n",
    "for entry in offsets_list:\n",
    "    ax[1].scatter(torch.rand(37).detach().cpu(), y=torch.arange(37).detach().cpu())\n",
    "    ax[1].set_xlabel(\"Chosen offset\")\n",
    "    ax[1].set_title(\"Random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abb5e51-e9e8-4328-83f1-bec9927aadc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstest\n",
    "\n",
    "def check_if_uniform(check_list):\n",
    "    for i, entry in enumerate(check_list.T):\n",
    "    # Example data\n",
    "        data = entry.cpu() # entrynp.random.randn((10)) #[0.1, 0.2, 0.35, 0.9, 0.65]  # Replace with your data\n",
    "        \n",
    "        # Perform K-S test against uniform distribution\n",
    "        statistic, p_value = kstest(data, 'uniform', args=(0, 1))\n",
    "        \n",
    "        #print(f\"K-S statistic: {statistic}\")\n",
    "        #print(f\"P-value: {p_value}\")\n",
    "        \n",
    "        # Interpret p-value\n",
    "        if p_value > 0.05:  # Common significance level is 0.05\n",
    "            #print(\"Fail to reject the null hypothesis: Data appears to be uniformly distributed.\")\n",
    "            continue\n",
    "        else:\n",
    "            print(i, \"Reject the null hypothesis: Data does not appear to be uniformly distributed.\")\n",
    "\n",
    "check_if_uniform(offsets_list.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba780a70-7a25-47df-ac97-6cab6f22c0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "mins, _ = uncompensated_parameters_selected[:, 0, 0, :].max(dim=0)\n",
    "print((uncompensated_parameters_selected[:, 0, 0, :] == mins).sum(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b475c34a-7816-4b23-aefb-e0263b68c228",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, entry in model.offset_space.items():\n",
    "    if isinstance(entry, MutableParameter):\n",
    "        value_list = [value.value_lims for label2, value in model.input_parameter_container.items() if isinstance(value, MutableParameter) and label==label2]\n",
    "        print(\"label:\", entry.value_lims, \"from\", value_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4619825c-6b7b-40f5-897b-abca776a6466",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_min_params, loss, loss_min_list = optimize_smart_walker(model, observed_rays, uncompensated_parameters_selected, iterations=1000, num_candidates=10000, step_width=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f9432e-9daa-4356-92d4-9e7120fa6bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.optimize import basinhopping\n",
    "\n",
    "def torch_to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy()\n",
    "\n",
    "def numpy_to_torch(array, device):\n",
    "    return torch.tensor(array, device=device, dtype=torch.float32)\n",
    "\n",
    "def objective_function(offsets_numpy, model, observed_rays, uncompensated_parameters):\n",
    "    # Convert numpy array to torch tensor\n",
    "    offsets = numpy_to_torch(offsets_numpy, device=model.device).unsqueeze(0)\n",
    "    #offsets = torch.clamp(offsets, 0, 1)\n",
    "    \n",
    "    # Evaluate the model's loss with these offsets\n",
    "    scaled_offsets = model.rescale_offset(offsets)\n",
    "    compensated_rays = model(uncompensated_parameters + scaled_offsets)\n",
    "    loss = ((compensated_rays - observed_rays) ** 2).mean().item()\n",
    "    return loss\n",
    "\n",
    "def optimize_smart_walker_scipy(model, observed_rays, uncompensated_parameters, iterations=100, hop_step=0.1, interval=1, stepsize=0.1):\n",
    "    # Convert initial parameters to numpy for scipy\n",
    "    initial_offsets = torch.rand(1, uncompensated_parameters.shape[-1], device=model.device)\n",
    "    initial_offsets_numpy = torch_to_numpy(initial_offsets).flatten()\n",
    "\n",
    "    # Define a list to track the minimum loss for each iteration\n",
    "    loss_min_list = []\n",
    "\n",
    "    # Set up the progress bar\n",
    "    pbar = tqdm(total=iterations)\n",
    "\n",
    "    def callback(x, f, accept):\n",
    "        # Update progress bar and log the loss\n",
    "        loss_min_list.append(f)\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix({\"loss\": f})\n",
    "\n",
    "    # Configure basinhopping with scipy\n",
    "    minimizer_kwargs = {\n",
    "        \"method\": \"Powell\",\n",
    "        \"args\": (model, observed_rays, uncompensated_parameters),\n",
    "        \"bounds\": [(0, 1)] * initial_offsets_numpy.size  # Bounds for each parameter\n",
    "    }\n",
    "    \n",
    "    # Run basinhopping\n",
    "    result = basinhopping(\n",
    "        objective_function, \n",
    "        initial_offsets_numpy, \n",
    "        niter=iterations,\n",
    "        T=hop_step,\n",
    "        stepsize=stepsize,\n",
    "        interval=interval,\n",
    "        minimizer_kwargs=minimizer_kwargs,\n",
    "        callback=callback,\n",
    "        disp=True\n",
    "    )\n",
    "\n",
    "    # Close the progress bar\n",
    "    pbar.close()\n",
    "    \n",
    "    # Convert the final result back to torch for further use\n",
    "    final_offsets_numpy = result.x\n",
    "    final_offsets_torch = numpy_to_torch(final_offsets_numpy, device=model.device).view(1, -1)\n",
    "    scaled_offsets = model.rescale_offset(final_offsets_torch.unsqueeze(1))\n",
    "    best_loss_params = uncompensated_parameters + scaled_offsets\n",
    "\n",
    "    return best_loss_params, result.fun, loss_min_list\n",
    "loss_min_params, loss, loss_min_list = optimize_smart_walker_scipy(model, observed_rays, uncompensated_parameters_selected, hop_step=0.1, iterations=1000, interval=10, stepsize=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380bf0f2-3bff-4f81-9fd4-63d8ed80d2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "def objective_function(offsets, model, observed_rays, uncompensated_parameters):\n",
    "    # Evaluate the model's loss with these offsets\n",
    "    scaled_offsets = model.rescale_offset(offsets)\n",
    "    compensated_rays = model(uncompensated_parameters + scaled_offsets)\n",
    "    loss = ((compensated_rays - observed_rays) ** 2).mean().item()\n",
    "    return loss\n",
    "\n",
    "def objective(trial):\n",
    "    param_list = torch.tensor([trial.suggest_float(str(i), 0, 1.) for i in range(37)], device=model.device)\n",
    "    return objective_function(param_list, model, observed_rays, uncompensated_parameters_selected)\n",
    "\n",
    "study = optuna.create_study()\n",
    "\n",
    "study.optimize(objective, n_trials=1000, show_progress_bar=True)\n",
    "\n",
    "study.best_params  # E.g. {'x': 2.002108042}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6538f0dc-1072-46c3-a16f-b9e54319dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ax import optimize\n",
    "import torch\n",
    "from ax.modelbridge.dispatch_utils import choose_generation_strategy\n",
    "from ax.modelbridge.generation_strategy import GenerationStep, GenerationStrategy\n",
    "from ax.modelbridge.modelbridge_utils import get_pending_observation_features\n",
    "from ax.modelbridge.registry import ModelRegistryBase, Models\n",
    "\n",
    "from ax.utils.testing.core_stubs import get_branin_experiment, get_branin_search_space\n",
    "\n",
    "def optimize_smart_walker_ax(model, observed_rays, uncompensated_parameters, iterations, num_initial_points=10):\n",
    "    \"\"\"\n",
    "    Optimize the smart walker using Bayesian Optimization via the Ax library.\n",
    "    \"\"\"\n",
    "    device = model.device\n",
    "\n",
    "    # Define the objective function\n",
    "    def objective_function(parameterization):\n",
    "        # Convert parameterization to tensor\n",
    "        offsets = torch.tensor([parameterization[f\"x{i}\"] for i in range(uncompensated_parameters.shape[-1])], device=device).unsqueeze(0)\n",
    "        \n",
    "        # Scale offsets and compute the compensated rays\n",
    "        scaled_offsets = model.rescale_offset(offsets)\n",
    "        compensated_rays = model(uncompensated_parameters + scaled_offsets)\n",
    "        \n",
    "        # Calculate the loss (mean squared error)\n",
    "        loss = ((compensated_rays - observed_rays) ** 2).mean().item()\n",
    "        print(loss)\n",
    "        return loss #{\"loss\": loss}  # Return loss as the objective value\n",
    "\n",
    "    # Define the parameter space for optimization\n",
    "    parameter_space = [\n",
    "        {\"name\": f\"x{i}\", \"type\": \"range\", \"bounds\": [0.0, 1.0], \"value_type\": \"float\"} \n",
    "        for i in range(uncompensated_parameters.shape[-1])\n",
    "    ]\n",
    "    gs = GenerationStrategy(\n",
    "        steps=[\n",
    "            # 1. Initialization step (does not require pre-existing data and is well-suited for\n",
    "            # initial sampling of the search space)\n",
    "            GenerationStep(\n",
    "                model=Models.SOBOL,\n",
    "                num_trials=5,  # How many trials should be produced from this generation step\n",
    "                min_trials_observed=3,  # How many trials need to be completed to move to next model\n",
    "                max_parallelism=5,  # Max parallelism for this step\n",
    "                model_kwargs={\"seed\": 999},  # Any kwargs you want passed into the model\n",
    "                model_gen_kwargs={},  # Any kwargs you want passed to `modelbridge.gen`\n",
    "            ),\n",
    "            # 2. Bayesian optimization step (requires data obtained from previous phase and learns\n",
    "            # from all data available at the time of each new candidate generation call)\n",
    "            GenerationStep(\n",
    "                model=Models.BOTORCH_MODULAR,\n",
    "                num_trials=-1,  # No limitation on how many trials should be produced from this step\n",
    "                max_parallelism=3,  # Parallelism limit for this step, often lower than for Sobol\n",
    "                # More on parallelism vs. required samples in BayesOpt:\n",
    "                # https://ax.dev/docs/bayesopt.html#tradeoff-between-parallelism-and-total-number-of-trials\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    gs = choose_generation_strategy(\n",
    "    # Required arguments:\n",
    "    search_space=get_branin_search_space(),  # Ax `SearchSpace`\n",
    "    # Some optional arguments (shown with their defaults), see API docs for more settings:\n",
    "    # https://ax.dev/api/modelbridge.html#module-ax.modelbridge.dispatch_utils\n",
    "    use_batch_trials=False,  # Whether this GS will be used to generate 1-arm `Trial`-s or `BatchTrials`\n",
    "    no_bayesian_optimization=False,  # Use quasi-random candidate generation without BayesOpt\n",
    "    max_parallelism_override=None,  # Integer, to which to set the `max_parallelism` setting of all steps in this GS\n",
    "    )\n",
    "\n",
    "    # Use Ax to optimize the objective function\n",
    "    best_parameters, values, experiment, surrogate = optimize(\n",
    "        parameters=parameter_space,\n",
    "        evaluation_function=objective_function,\n",
    "        minimize=True,  # Minimize the loss\n",
    "        total_trials=iterations,\n",
    "        random_seed=42,  # Ensure reproducibility\n",
    "        #init_trials=num_initial_points,  # Number of random initial samples\n",
    "        generation_strategy = gs\n",
    "    )\n",
    "\n",
    "    # Extract the best found parameters and convert to tensor\n",
    "    best_offsets = torch.tensor([best_parameters[f\"x{i}\"] for i in range(uncompensated_parameters.shape[-1])], device=device).unsqueeze(0)\n",
    "    scaled_offsets = model.rescale_offset(best_offsets)\n",
    "    loss_min_params = uncompensated_parameters + scaled_offsets\n",
    "    loss_min = values[0]\n",
    "\n",
    "    return loss_min_params, loss_min, experiment\n",
    "\n",
    "loss_min_params, loss, loss_min_list = optimize_smart_walker_ax(model, observed_rays, uncompensated_parameters_selected, iterations=1000, num_initial_points=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eebe1b-8b00-494f-98af-35129d60a816",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_min_params, loss, loss_min_list = optimize_pso(model, observed_rays, uncompensated_parameters_selected, iterations=1000, num_candidates=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1ecd44-20af-4a4d-8640-37dc46969f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_min_params, loss, loss_min_list = optimize_ea(model, observed_rays, uncompensated_parameters_selected, iterations=1000, num_candidates=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f3671d-08ec-44a9-98c4-d2996f0b389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_min_params, loss, loss_min_list = optimize_brute(model, observed_rays, uncompensated_parameters_selected, iterations=1000, num_candidates=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f650064-9e24-4a1b-ac4a-7a60d4d0dcc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_min_params, loss, loss_min_list = optimize_smart_walker(model, observed_rays, uncompensated_parameters_selected, iterations = 1000, num_candidates=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7484665b-0022-439e-850d-39a61aba177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = plot_param_tensors(loss_min_params[[1,2,4,8]], uncompensated_parameters_selected[[1,2,4,8]], engine = engine, ray_parameter_container=model.input_parameter_container, compensated_parameters=compensated_parameters_selected[[1,2,4,8]])\n",
    "\n",
    "method_dict = {\"smart walker\": (optimize_smart_walker, 1000), \"brute\": (optimize_brute, 1000), \"pso\": (optimize_pso, 1000), \"ea\": (optimize_ea, 1000)}\n",
    "method_evaluation_list = []\n",
    "\n",
    "for key, entry in tqdm(method_dict.items(), desc=\"Evaluating methods\"):\n",
    "    mean_best, std_best, mean_progress, std_progress, loss_min_params_tens = evaluate_evaluation_method(entry[0], model, observed_rays, uncompensated_parameters_selected, offsets_selected, repetitions=2, num_candidates=entry[1], iterations=1000)\n",
    "    method_evaluation_list.append((key, mean_best, std_best, mean_progress, std_progress))\n",
    "\n",
    "    # calculate deviations from target offset\n",
    "    #print(\"offsets\", offsets_selected.shape)\n",
    "    #print(\"loss_min_params_tens\", loss_min_params_tens.shape, \"uncompensated_parameters_selected\", uncompensated_parameters_selected[0].shape)\n",
    "    predicted_offsets = (loss_min_params_tens[:, 0] - uncompensated_parameters_selected[0, 0])\n",
    "    #print(predicted_offsets.min(), predicted_offsets.max())\n",
    "    normalized_predicted_offsets = model.unscale_offset(predicted_offsets)\n",
    "    #print(normalized_predicted_offsets.min(), normalized_predicted_offsets.max())\n",
    "    rmse = ((offsets_selected-normalized_predicted_offsets)**2).mean().sqrt().item()\n",
    "    #print(key, \":\", mean_best, \"±\", std_best, \"RMSE from target offset:\", rmse)\n",
    "    #print(\"unc-pred\",uncompensated_parameters_selected.shape, predicted_offsets.shape)\n",
    "    loss_min_params_tens = (uncompensated_parameters_selected + predicted_offsets).swapaxes(0,1)\n",
    "    #print(loss_min_params_tens.flatten(start_dim=0, end_dim=1))\n",
    "    loss_min_params_tens = loss_min_params_tens.flatten(start_dim=0, end_dim=1)\n",
    "    continue\n",
    "    loss_min_ray_outputs = param_tensor_to_ray_outputs(loss_min_params_tens, engine, model.input_parameter_container)\n",
    "    reference_ray_outputs = param_tensor_to_ray_outputs(compensated_parameters_selected.swapaxes(0,1), engine, model.input_parameter_container)\n",
    "    compensated_parameters_selected_ray_outputs = param_tensor_to_ray_outputs(compensated_parameters_selected.swapaxes(0,1).repeat_interleave(10, dim=0), engine, model.input_parameter_container)\n",
    "    out = compare_with_reference(reference_ray_outputs, loss_min_ray_outputs)\n",
    "    print(\"deviation best to ref\", out[0], \"±\", out[1])\n",
    "    out = compare_with_reference(reference_ray_outputs, compensated_parameters_selected_ray_outputs)\n",
    "    print(\"deviation ref to ref\", out[0], \"±\", out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d099fa3-e97e-43f1-ba79-78a0aad7e797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from geomloss import SamplesLoss  # GeomLoss for Sinkhorn distance\n",
    "\n",
    "# Step 1: Create two Gaussian point clouds\n",
    "def create_gaussian_blob(center, cov, num_points=100):\n",
    "    mean = center\n",
    "    cov_matrix = torch.diag(torch.tensor(cov))  # Covariance matrix for Gaussian\n",
    "    distribution = torch.distributions.MultivariateNormal(mean, cov_matrix)\n",
    "    points = distribution.sample((num_points,))\n",
    "    return points\n",
    "\n",
    "# Create two Gaussian blobs\n",
    "blob1 = create_gaussian_blob(center=torch.tensor([0.0, 0.0]), cov=[1.0, 1.0], num_points=100)\n",
    "blob2 = create_gaussian_blob(center=torch.tensor([3.0, 3.0]), cov=[1.0, 1.0], num_points=100)\n",
    "\n",
    "# Step 2: Define the Sinkhorn distance using GeomLoss\n",
    "loss_fn = SamplesLoss(\"sinkhorn\", blur=0.1)  # Sinkhorn distance with regularization\n",
    "\n",
    "# Step 3: Compute the Sinkhorn distance\n",
    "sinkhorn_distance = loss_fn(blob1, blob2)\n",
    "\n",
    "# Print the resulting Sinkhorn distance\n",
    "print(f\"Sinkhorn distance between the two blobs: {sinkhorn_distance.item()}\")\n",
    "\n",
    "# Step 4: (Optional) Visualize the point clouds\n",
    "plt.scatter(blob1[:, 0].cpu(), blob1[:, 1].cpu(), color='red', label='Blob 1', alpha=0.6)\n",
    "plt.scatter(blob2[:, 0].cpu(), blob2[:, 1].cpu(), color='blue', label='Blob 2', alpha=0.6)\n",
    "plt.title(\"2D Gaussian Blobs\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c259ac6-0afd-4e82-9bbf-36a39d0cd3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = SinkhornLoss()\n",
    "sl.loss_fn(out[0], out[1], 'ImagePlane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8916a065-2c8f-4f19-ad9e-ba5783df0286",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6.905, 4.434))\n",
    "ax = plt.gca()\n",
    "i = 0\n",
    "plot_list = []\n",
    "for key, mean_best, std_best, mean_progress, std_progress in method_evaluation_list:\n",
    "    color = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"][i+3]\n",
    "    plt.fill_between(torch.arange(len(mean_progress)), (mean_progress-std_progress).cpu(), (mean_progress+std_progress).cpu(), color=color, alpha=0.2)\n",
    "    plot, = plt.plot(torch.arange(len(mean_progress)), mean_progress.cpu(), alpha = 1., c = color)\n",
    "    plot_list.append(plot)\n",
    "    i = i+1\n",
    "ax.legend(plot_list, [key for key in method_dict.keys()], prop={'size': 11})\n",
    "ax.tick_params(axis='both', which='major', labelsize=11)\n",
    "plt.xlabel('Iteration', fontsize=16)\n",
    "plt.ylabel('MSE (log)', fontsize=16)\n",
    "ax.set_yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/bl_optimizer_iterations.pdf', bbox_inches='tight', pad_inches = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4e4a4b-2460-44ab-82fa-da1a5bb75243",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = benchmark.Timer(\n",
    "    stmt='optimize_smart_walker(model, observed_rays, uncompensated_parameters_selected)',\n",
    "    setup='from __main__ import optimize_smart_walker',\n",
    "    globals={'model': model, 'observed_rays': observed_rays, 'uncompensated_parameters_selected': uncompensated_parameters_selected},\n",
    "    num_threads=1,\n",
    "    label='optimize smart walker',\n",
    "    sub_label='optimize smart walker')\n",
    "print(t0.timeit(repetitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4765ab94-0f73-4c31-b52d-b11ae0bfe002",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_observed_rays = observed_rays.squeeze()\n",
    "plot_min_param_rays = model(loss_min_params)\n",
    "fig, ax = plt.subplots(1, plot_observed_rays.shape[0], sharex=True, sharey=True, figsize=(32, 9))\n",
    "with torch.no_grad():\n",
    "    for i in range(plot_observed_rays.shape[0]):\n",
    "        ax[i].plot(model(loss_min_params)[i].cpu())\n",
    "        ax[i].plot(observed_rays.squeeze()[i].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d25718-4846-4cce-98f4-a7dcf1dd4298",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_min_params.shape\n",
    "from hist_optimizer import simulate_param_tensor\n",
    "\n",
    "out = [simulate_param_tensor(loss_min_params[:, i], engine, model.input_parameter_container) for i in range(loss_min_params.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b24b186-9011-4d51-a703-e160802abebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_tensors(loss_min_params[:,0], uncompensated_parameters_selected[:,0], engine = engine, ray_parameter_container=model.input_parameter_container, compensated_parameters=compensated_parameters_selected[:,0], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8068af62-28f8-448e-a64d-9eea112b9a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_param_tensor: tensor with dim [beamline_configs, z_layers, beamline_parameters]\n",
    "from sub_projects.ray_optimization.utils import ray_output_to_tensor, ray_dict_to_tensor\n",
    "def simulate_param_tensor(input_param_tensor, engine, ray_parameter_container, exported_plane='ImagePlane'):\n",
    "    assert len(input_param_tensor.shape) == 3\n",
    "    pc = tensor_list_to_param_container_list(input_param_tensor[:,0], ray_parameter_container)\n",
    "    for entry in pc:\n",
    "        entry[exported_plane+'.translationZerror'].value = 0.\n",
    "\n",
    "    z_min, z_max = ray_parameter_container[exported_plane+'.translationZerror'].value_lims\n",
    "    z_layers = input_param_tensor[0,:,-1] * (z_max-z_min) + z_min\n",
    "    z_layer_list = z_layers.tolist()\n",
    "    \n",
    "    engine_output = engine.run(pc, MultiLayer(z_layer_list))\n",
    "    return [ray_dict_to_tensor(entry, \"ImagePlane\", to_cpu=True) for entry in engine_output]\n",
    "out = simulate_param_tensor(loss_min_params, engine, ray_parameter_container=model.input_parameter_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca2dfaf-7a6b-4d4a-a66c-1b0c9f4e2330",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = fancy_plot_param_tensors(loss_min_params[:], uncompensated_parameters_selected[:].squeeze(), engine = engine, ray_parameter_container=model.input_parameter_container, compensated_parameters=compensated_parameters_selected[:].squeeze())\n",
    "import plotly.io as pio\n",
    "\n",
    "# Save the figure to an HTML file\n",
    "pio.write_html(fig, 'figure.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060356b2-e008-4493-881b-75efef1252dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray_tools.base.transform import Histogram, RayTransformConcat, XYHistogram\n",
    "\n",
    "transforms = [\n",
    "        XYHistogram(50, (-10., 10.), (-3., 3.))\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, sharey=True, squeeze=False)\n",
    "\n",
    "x_simulation_hist_list = []\n",
    "y_simulation_hist_list = []\n",
    "for i in trange(2):\n",
    "    out = engine.run(tensor_list_to_param_container_list(loss_min_params), XYHistogram(50, (-10., 10.), (-3., 3.)))\n",
    "    out_simulation = out[-1]['ray_output']['ImagePlane']['0.0']\n",
    "    x_simulation_hist, _ = torch.histogram(out_simulation.x_loc,bins=50, range=[-10, 10])\n",
    "    y_simulation_hist, _ = torch.histogram(out_simulation.y_loc,bins=50, range=[-3, 3])\n",
    "    x_simulation_hist_list.append(x_simulation_hist / 22594.)\n",
    "    y_simulation_hist_list.append(y_simulation_hist / 22594.)\n",
    "    \n",
    "    ax[0, 0].plot(torch.linspace(-10, 10, 50), x_simulation_hist / 22594.)\n",
    "    ax[0, 1].plot(torch.linspace(-3, 3, 50), y_simulation_hist / 22594.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2835c97-20fc-4df7-9f82-e8a0460905bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_min_params.shape\n",
    "t = tensor_list_to_param_container_list(loss_min_params)\n",
    "out = [engine.run(tensor_list_to_param_container_list(loss_min_params), XYHistogram(50, (-10., 10.), (-3., 3.))) for i in trange(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dee67c-a53d-48d2-847e-a2dc43d1673a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be920a4b-df8d-4c6a-afd3-ee2860c7ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f93da-f748-4d9a-8cc9-7d83236cd629",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_list = []\n",
    "for i in out:\n",
    "    hist_list.append(torch.vstack([j['ray_output']['ImagePlane']['histogram'].reshape(-1) / 22594. for j in i]))\n",
    "#        plt.plot(j['ray_output']['ImagePlane']['histogram'].reshape(-1) / 22594.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97df58e-a7a2-4e45-a925-63b0bd0d6d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_list_tensor = torch.stack(hist_list)\n",
    "#print(hist_list_tensor.shape)\n",
    "print(\"var\",hist_list_tensor.var(dim=0, correction=0).mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4318fe85-2d93-4312-ac5a-b089c8f9c707",
   "metadata": {},
   "source": [
    "# Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf51d90-a077-4aea-98dd-d88b38fc077f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_len: int | None = None #10000\n",
    "h5_files = list(glob.iglob('datasets/metrix_simulation/ray_emergency_surrogate_50+50+z/histogram_*.h5'))\n",
    "    sub_groups = ['parameters', 'histogram/ImagePlane', 'n_rays/ImagePlane']\n",
    "    transforms=[lambda x: x[1:].float(), lambda x: standardizer(x.flatten().float()), lambda x: x.int()]\n",
    "    dataset = HistDataset(h5_files, sub_groups, transforms, normalize_sub_groups=['parameters'], load_max=load_len)\n",
    "\n",
    "\n",
    "bal_memory_dataset = BalancedMemoryDataset(dataset=dataset, load_len=load_len, min_n_rays=1)\n",
    "memory_dataset = MemoryDataset(dataset=dataset, load_len=load_len)\n",
    "datamodule = DefaultDataModule(dataset=bal_memory_dataset, num_workers=4)\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup(stage=\"test\")\n",
    "test_dl = datamodule.test_dataloader()\n",
    "\n",
    "unbal_datamodule = DefaultDataModule(dataset=memory_dataset, num_workers=4)\n",
    "unbal_datamodule.prepare_data()\n",
    "unbal_datamodule.setup(stage=\"test\")\n",
    "unbal_test_dl = unbal_datamodule.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98253db-b210-4ab2-983f-40c050995119",
   "metadata": {},
   "source": [
    "## Maximum distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b4e764-e7da-4764-9e8c-a66a8472e46f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "value_list = []\n",
    "params_list = []\n",
    "\n",
    "for i in tqdm(unbal_test_dl):\n",
    "    biggest = i[1].flatten(start_dim=1)\n",
    "    biggest, _ = i[1].flatten(start_dim=1).max(dim=1)\n",
    "    mask = biggest > 0.8\n",
    "    value_list.append(biggest[mask])\n",
    "    params_list.append(i[0][mask])\n",
    "value_tensor = torch.cat(value_list)\n",
    "params_tensor = torch.cat(params_list)\n",
    "\n",
    "torch.save(value_tensor, 'outputs/values.pt')\n",
    "torch.save(params_tensor, 'outputs/params.pt')\n",
    "plt.hist(value_tensor)\n",
    "plt.savefig('outputs/max_dist_hist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58de822f-7b2f-4b1b-8ce3-5075f5de708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_tensor = torch.load('outputs/values.pt')\n",
    "params_tensor = torch.load('outputs/params.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdc65fd-e313-4628-9287-388b7e95c562",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(test_dl):\n",
    "    #print(len(i[0][:10]))\n",
    "    t = tensor_list_to_param_container_list(i[0][:10])\n",
    "    print(len(t))\n",
    "    out = [engine.run(t, XYHistogram(50, (-10., 10.), (-3., 3.))) for i in trange(2)]\n",
    "    break\n",
    "#['ray_output']['ImagePlane']['histogram']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b620b358-ba48-49a2-ad0f-89e4a3dca9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out[0][0]['ray_output']['ImagePlane']['histogram'])\n",
    "len(out), len(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cb59fd-09a9-4d71-9f60-a35e971980d5",
   "metadata": {},
   "source": [
    "# Special sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a67b85-035a-4433-b00f-80af733a4479",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"outputs/special_sample_168_selected.pkl\", \"rb\") as f:\n",
    "    special_sample = pickle.load(f, fix_imports=True, encoding='ASCII', errors='strict', buffers=None)\n",
    "observed_params = special_sample.uncompensated_parameters\n",
    "\n",
    "for param_container in observed_params:\n",
    "    for label in ['ImagePlane.translationXerror', 'ImagePlane.translationYerror', 'ImagePlane.translationZerror']:\n",
    "        if label in list(param_container.keys()):\n",
    "            del param_container[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f574994c-891f-4241-8ebc-69923812263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(observed_params)\n",
    "Plot.plot_engines_comparison(engine, surrogate_engine, observed_params[:5], MultiLayer([0.]), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aedafb7-548b-47ee-bdb1-a33c346eda44",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncompensated_parameters = [elem.clone() for elem in special_sample.uncompensated_parameters]\n",
    "for elem in uncompensated_parameters:\n",
    "    elem.perturb(special_sample.target_params)\n",
    "uncompensated_parameters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903329a4-b9af-4627-a406-8d93a8b058f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot.plot_engines_comparison(engine, surrogate_engine, uncompensated_parameters, MultiLayer([0.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f251daa-c4f3-4461-af8d-37f6522f99c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_comparison, x_simulation_hist, y_simulation_hist = mse_engines_comparison(engine, surrogate_engine, uncompensated_parameters[:5], MultiLayer([0.]))\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "for hist in x_simulation_hist:\n",
    "    ax[0].plot(hist, alpha=0.3)\n",
    "for hist in y_simulation_hist:\n",
    "    ax[1].plot(hist, alpha=0.3)\n",
    "ax[2].hist(mse_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a5863a-6470-42a4-9f13-3f7b5abe0509",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_loc_list = []\n",
    "good_param_list = []\n",
    "batch_size = 5000\n",
    "for i in trange(15000//batch_size):\n",
    "    param_container = [tensor_to_param_container(torch.rand((34,))) for _ in range(batch_size)]\n",
    "    surrogate_out = surrogate_engine.run(param_container, MultiLayer([0.]))\n",
    "    for j in range(len(param_container)):\n",
    "        output = surrogate_out[j]['ray_output']['ImagePlane']['xy_hist']\n",
    "        if output.x_loc.sum() > 0.5:\n",
    "            x_loc_list.append(output.x_loc.sum())\n",
    "            good_param_list.append(param_container[j])\n",
    "\n",
    "observed_containers_tensor = torch.vstack([surrogate_engine.select({\"1e5/params\":param_container})[0] for param_container in observed_params])\n",
    "good_containers_tensor = torch.vstack([surrogate_engine.select({\"1e5/params\":param_container})[0] for param_container in good_param_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a886aeb-f0b4-4971-84e8-4b2b3662ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "for i in range(good_containers_tensor.shape[0]):\n",
    "    plt.plot(good_containers_tensor[i], c = 'blue', alpha=0.1)\n",
    "for i in range(observed_containers_tensor.shape[0]):\n",
    "    plt.plot(observed_containers_tensor[i], alpha=0.8)\n",
    "plt.legend([\"special\", \"good\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418cb63a-b773-4aab-854b-dec4755ccc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask = value_tensor > 0.44\n",
    "out = ((params_tensor - observed_containers_tensor[0].unsqueeze(0))**2)/2.\n",
    "out = out.mean(dim=1)\n",
    "out_sorted, indices = torch.sort(out)\n",
    "#part_indices = indices[:5]\n",
    "#print(part_indices.shape)\n",
    "#min_arg = out.argmin()\n",
    "#plt.hist(out.mean(dim=1))\n",
    "#plt.plot(params_tensor[min_arg])\n",
    "#plt.plot(observed_containers_tensor[0])\n",
    "for i in indices[:1]:\n",
    "    plt.plot(params_tensor[i])\n",
    "Plot.plot_engines_comparison(engine, surrogate_engine, [tensor_to_param_container(params_tensor[min_arg]) for min_arg in indices[:1]], MultiLayer([0.]), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e4311-ab69-4692-a340-5f0cb3342039",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = {\"ImagePlane\": transform for transform in cfg_transforms}\n",
    "out_engine = engine.run(observed_params, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636cd240-dc8b-48bf-937c-7c39caa881fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_simulations = surrogate_engine.model.standardizer(torch.vstack([element['ray_output']['ImagePlane']['histogram'].flatten(start_dim=0) for element in out_engine]))\n",
    "a = ((standardized_simulations - out_model)**2).mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7500019-8d32-4055-b085-3311b7bdeb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a933e278-11bf-4216-bdb2-e6643ed5d668",
   "metadata": {},
   "source": [
    "# Good params vs. bad params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7513437-7f9c-44b0-8793-e9549a488ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list = []\n",
    "num_rays_list = []\n",
    "for i in tqdm(memory_dataset):\n",
    "    params_list.append(i[0])\n",
    "    num_rays_list.append(i[2])\n",
    "params_tensor = torch.vstack(params_list)\n",
    "num_rays_tensor= torch.vstack(num_rays_list)\n",
    "plt.hist(torch.tensor(num_rays_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0f571f-4de1-43c0-b71e-df26e91d5d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest = torch.tensor(num_rays_list).argmax()\n",
    "test_parameters = memory_dataset[biggest][0]\n",
    "param_container_list = [tensor_to_param_container(test_parameters)]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, sharey=True, squeeze=False)\n",
    "\n",
    "x_simulation_hist_list = []\n",
    "y_simulation_hist_list = []\n",
    "for i in trange(20):\n",
    "    out = engine.run(param_container_list, MultiLayer([0.]))\n",
    "    out_simulation = out[-1]['ray_output']['ImagePlane']['0.0']\n",
    "    x_simulation_hist, _ = torch.histogram(out_simulation.x_loc,bins=50, range=[-10, 10])\n",
    "    y_simulation_hist, _ = torch.histogram(out_simulation.y_loc,bins=50, range=[-3, 3])\n",
    "    x_simulation_hist_list.append(x_simulation_hist / 22594.)\n",
    "    y_simulation_hist_list.append(y_simulation_hist / 22594.)\n",
    "    \n",
    "    ax[0, 0].plot(torch.linspace(-10, 10, 50), x_simulation_hist / 22594.)\n",
    "    ax[0, 1].plot(torch.linspace(-3, 3, 50), y_simulation_hist / 22594.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aed3db-467f-48ef-8886-f39faab1cc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_simulation_hist_tens = torch.vstack(x_simulation_hist_list)\n",
    "y_simulation_hist_tens = torch.vstack(y_simulation_hist_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f043fc52-8c01-487f-a2f8-cef90a8be1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_simulation_hist_tens.var(dim=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc62cae5-f272-41db-b531-ca271b79475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_simulation_hist_tens.mean(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8463884f-efac-4924-acb9-bcf050cc3494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "mask = num_rays_tensor > 100.\n",
    "data_tensor = params_tensor[mask.flatten()]\n",
    "data_tensor = data_tensor[:,:4]\n",
    "class_tensor = num_rays_tensor[mask]\n",
    "\n",
    "data_np = data_tensor.numpy()\n",
    "class_np = class_tensor.numpy().flatten()\n",
    "\n",
    "umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2)\n",
    "umap_embedding = umap_model.fit_transform(data_np)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(umap_embedding[:, 0], umap_embedding[:, 1], c=class_np, cmap='Spectral', s=5)\n",
    "plt.colorbar(scatter)\n",
    "plt.title('UMAP projection of the dataset')\n",
    "plt.xlabel('UMAP 1')\n",
    "plt.ylabel('UMAP 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0bf724-e188-4443-b1c9-bcc265cb445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne_model = TSNE(n_components=2, perplexity=30, learning_rate=200, max_iter=1000)\n",
    "tsne_embedding = tsne_model.fit_transform(data_np)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(tsne_embedding[:, 0], tsne_embedding[:, 1], c=class_np, cmap='Spectral', s=5)\n",
    "plt.colorbar(scatter)\n",
    "plt.title('t-SNE projection of the dataset')\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a61f44c-0ba3-4eec-97c2-31fdeac4cb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from ray_nn.nn.xy_hist_data_models import MetrixXYHistSurrogate, StandardizeXYHist\n",
    "from ray_tools.simulation.torch_datasets import MemoryDataset, HistDataset\n",
    "from datasets.metrix_simulation.config_ray_emergency_surrogate import PARAM_CONTAINER_FUNC as params\n",
    "from torch.utils.data import DataLoader\n",
    "from ray_nn.data.transform import Select\n",
    "\n",
    "model_path = \"../../outputs/xy_hist/ee8cvj82/checkpoints/epoch=36-step=35212012.ckpt\"\n",
    "model = MetrixXYHistSurrogate.load_from_checkpoint(model_path)\n",
    "model.to(torch.device('cpu'))\n",
    "model.compile()\n",
    "model.eval()\n",
    "\n",
    "load_len: int | None = 1000\n",
    "h5_files = list(glob.iglob('datasets/metrix_simulation/ray_emergency_surrogate_50+50+z/histogram_*.h5'))\n",
    "sub_groups = ['parameters', 'histogram/ImagePlane', 'n_rays/ImagePlane']\n",
    "transforms=[lambda x: x[1:].float(), lambda x: standardizer(x.flatten().float()), lambda x: x.int()]\n",
    "dataset = HistDataset(h5_files, sub_groups, transforms, normalize_sub_groups=['parameters'], load_max=load_len)\n",
    "\n",
    "\n",
    "memory_dataset = MemoryDataset(dataset=dataset, load_len=load_len)\n",
    "\n",
    "train_dataloader = DataLoader(memory_dataset, batch_size=2048, shuffle=False, num_workers=0)\n",
    "\n",
    "errors_list = []\n",
    "with torch.no_grad():\n",
    "    for par_input, label, _ in tqdm(train_dataloader):\n",
    "        out = model(par_input)\n",
    "        label = label.flatten(start_dim=1)\n",
    "        b = ((label - out)**2).mean(dim=1)\n",
    "        errors_list.append(b)\n",
    "errors_tensor = torch.cat(errors_list)\n",
    "\n",
    "plt.hist(errors_tensor)\n",
    "plt.savefig('outputs/dataset_errors_hist.png')\n",
    "torch.save(errors_tensor, 'outputs/dataset_errors.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26899f5-568d-4447-af06-fc8786606588",
   "metadata": {},
   "source": [
    "# Import real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2faae3b-c24d-42dc-89fd-7043dd4eaf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_real_hist_data(parameter_container, device, path = '../../datasets/metrix_real_data/2021_march_complete', import_set = ['M03', 'M10', 'M18', 'M22', 'M23', 'M24', 'M25', 'M27', 'M28', 'M29', 'M30', 'M32', 'M33', 'M36',\n",
    "                             'M37', 'M40', 'M41', 'M42', 'M43', 'M44'], z_layers=[0., 5., 10.], check_value_lims=False, z_array_label='ImagePlane.translationZerror'):\n",
    "    imported_data = import_data(\n",
    "                path,\n",
    "                import_set,\n",
    "                z_layers,\n",
    "                parameter_container,\n",
    "                check_value_lims=check_value_lims,\n",
    "            )\n",
    "    xy_hist = XYHistogram(50, (-10., 10.), (-3., 3.))\n",
    "    z_array_min, z_array_max = model.input_parameter_container[z_array_label].value_lims\n",
    "    normalized_z_array = torch.tensor((z_layers - z_array_min) / (z_array_max - z_array_min), device=device).float()\n",
    "\n",
    "    # observed_rays_point_cloud\n",
    "    real_data_point_cloud_list = []\n",
    "    for i in range(len(imported_data)):\n",
    "        real_data_point_cloud_list.append(imported_data[i])\n",
    "    observed_rays_point_cloud = [ray_dict_to_tensor(entry, 'ImagePlane') for entry in real_data_point_cloud_list]\n",
    "\n",
    "    real_data_list = []\n",
    "    for i in range(len(imported_data)):\n",
    "        real_data_list.append(xy_hist(imported_data[i]['ray_output']['ImagePlane']))\n",
    "    z_layer_real_data_tensor_list = []\n",
    "    for z in z_layers:\n",
    "        z_layer_real_data_tensor_list.append(torch.stack([real_data_list[i][z]['histogram'] for i in range(len(real_data_list))]))\n",
    "    real_data_tensor = torch.stack(z_layer_real_data_tensor_list, dim=1)\n",
    "\n",
    "    real_data_tensor_normalized = real_data_tensor / 4000. #surrogate_engine.model.standardizer(real_data_tensor)\n",
    "    observed_rays = real_data_tensor_normalized.flatten(start_dim=-2).unsqueeze(2).float().to(device)\n",
    "\n",
    "    uncompensated_parameters_list = []\n",
    "    for i in range(len(imported_data)):\n",
    "        uncompensated_entry = torch.tensor([value.get_value() for value in Plot.normalize_parameters(imported_data[i]['param_container_dict'], parameter_container).values()])\n",
    "        uncompensated_parameters_list.append(uncompensated_entry)\n",
    "    uncompensated_parameters = torch.stack(uncompensated_parameters_list)\n",
    "    uncompensated_parameters = uncompensated_parameters.unsqueeze(1).float().to(device)\n",
    "    uncompensated_parameters = uncompensated_parameters.repeat_interleave(len(z_layers), dim=1).unsqueeze(2)\n",
    "    uncompensated_parameters[:, :, 0, -1] = normalized_z_array\n",
    "    \n",
    "    #print(uncompensated_parameters)\n",
    "    \n",
    "    return observed_rays, uncompensated_parameters, observed_rays_point_cloud\n",
    "    \n",
    "observed_rays_real, uncompensated_parameters_real, observed_rays_point_cloud = import_real_hist_data(model.input_parameter_container, device=model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc58b100-897d-45a2-aa64-73361f5b6c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_min_params = optimize_smart_walker(model, observed_rays_real, uncompensated_parameters_real, iterations=1000, num_candidates=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ec1683-db55-411e-94f5-c232488db214",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device('cpu')\n",
    "pc = [tensor_to_param_container(loss_min_params[i].cpu()) for i in range(loss_min_params.shape[0])]\n",
    "Plot.plot_engines_comparison(engine, surrogate_engine, pc[:8], MultiLayer([0.]), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb2c876-88bd-41e1-a73c-088c9b82b9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f2f2b8-bac3-49ac-98eb-3d8fc157054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_tensors(loss_min_params[:], uncompensated_parameters_selected[:], compensated_parameters=compensated_parameters_selected[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccf28d6-559e-4c95-bc95-360d71a6eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_params = torch.vstack(10*[loss_min_params[1]])\n",
    "a = repeated_params.clone()\n",
    "b = repeated_params.clone()\n",
    "c = repeated_params.clone()\n",
    "for i, ten in enumerate(a):\n",
    "    ten[-1]=0.5+0.1*i\n",
    "    ten[-2]=0.5\n",
    "for i, ten in enumerate(b):\n",
    "    ten[-2]=0.5+0.1*i\n",
    "    ten[-1]=0.5\n",
    "for i, ten in enumerate(c):\n",
    "    ten[-1]=0.5\n",
    "    ten[-2]=0.5#+0.005*i\n",
    "plot_param_tensors(a[:3], b[:3], compensated_parameters=c[:3])\n",
    "#print(repeated_params)\n",
    "#plot_param_tensors(loss_min_params[:], uncompensated_parameters_selected[:], compensated_parameters=compensated_parameters_selected[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a87724d-4ca2-433d-9957-e4f8ae902631",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_min_params.shape, uncompensated_parameters_selected.shape, compensated_parameters_selected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0e49fe-5a57-4edf-a8f4-fab2f552d124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ef0d19-7c06-4a60-acc3-b5114c15cd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_comparison_plot = Plot.plot_param_comparison(\n",
    "    predicted_params=tensor_list_to_param_container_list(loss_min_params.squeeze().unsqueeze(0)),\n",
    "    epoch=42,\n",
    "    training_samples_count=len(observed_rays),\n",
    "    search_space=model.offset_space,\n",
    "    real_params=tensor_list_to_param_container_list(compensated_parameters_selected)[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b217c54c-b558-41f1-9e7a-66b78f1bdc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_comparison_plot = Plot.plot_param_comparison(\n",
    "    predicted_params=tensor_to_param_container(offsets_selected.squeeze()),\n",
    "    epoch=42,\n",
    "    training_samples_count=len(observed_rays),\n",
    "    search_space=RayOptimization.limited_search_space(model.offset_space, RandomGenerator(42), max_deviation=max_offset),\n",
    "    real_params=tensor_list_to_param_container_list(loss_min_params[0] - compensated_parameters_selected.squeeze())[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c91816-b241-4f86-ba22-b4f610b91e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import transforms\n",
    "import numpy as np\n",
    "from matplotlib.layout_engine import ConstrainedLayoutEngine, TightLayoutEngine\n",
    "from matplotlib.ticker import NullLocator\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "rs = np.random.RandomState(11)\n",
    "x = rs.gamma(4, size=1000)\n",
    "y = -.5 * x + rs.normal(size=1000)\n",
    "x = x / 4  -1\n",
    "y = y/4 +1\n",
    "\n",
    "\n",
    "def scatter_hist(x, y, ax, ax_histx, ax_histy):\n",
    "    xlim_min, xlim_max, ylim_min, ylim_max = -2, 2, -2, 2\n",
    "    # no labels\n",
    "    ax_histx.tick_params(axis=\"x\", labelbottom=False)\n",
    "    ax_histy.tick_params(axis=\"y\", labelleft=False)\n",
    "\n",
    "    # the scatter plot:\n",
    "    color = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"][1]\n",
    "    ax.scatter(\n",
    "    x,\n",
    "    y,\n",
    "    s=2.0,\n",
    "    alpha=0.5,\n",
    "    linewidths=0.4,\n",
    "    color=color\n",
    "    )\n",
    "    ax.set_xlim(\n",
    "    Plot.scale_interval(\n",
    "        xlim_min, xlim_max, 1.2\n",
    "    )\n",
    ")\n",
    "    ax.set_ylim(\n",
    "        Plot.scale_interval(\n",
    "            ylim_min, ylim_max, 1.2\n",
    "        )\n",
    "    )\n",
    "    ax.tick_params(axis=\"both\", length=0.0)\n",
    "    ax.grid(linestyle=\"dashed\", alpha=0.5)\n",
    "    ax.xaxis.set_major_locator(NullLocator())\n",
    "    ax.yaxis.set_major_locator(NullLocator())\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter(\"%.1f\"))\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter(\"%.1f\"))\n",
    "    \n",
    "    ax.set_xticks((xlim_min, xlim_max))\n",
    "    ax.set_yticks((ylim_min, ylim_max))\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "\n",
    "    # now determine nice limits by hand:\n",
    "    binwidth = 0.25\n",
    "    xymax = max(np.max(np.abs(x)), np.max(np.abs(y)))\n",
    "    lim = (int(xymax/binwidth) + 1) * binwidth\n",
    "\n",
    "    bins = np.arange(-lim, lim + binwidth, binwidth)\n",
    "    #ax_histx.hist(x, bins=bins, color=color)\n",
    "    x_simulation_hist, _ = torch.histogram(torch.from_numpy(x),bins=50, range=[-2, 2])\n",
    "    ax_histx.plot(torch.linspace(-2, 2, 50), x_simulation_hist, color=color)\n",
    "    ax_histx.set_yticklabels([])\n",
    "    #ax_histy.hist(y, bins=bins, orientation='horizontal', color=color)\n",
    "    y_simulation_hist, _ = torch.histogram(torch.from_numpy(y),bins=50, range=[-2, 2])\n",
    "    ax_histy.plot(y_simulation_hist, torch.linspace(-2, 2, 50), color=color)\n",
    "    ax_histy.set_xticklabels([])\n",
    "\n",
    "\n",
    "# Create a Figure, which doesn't have to be square.\n",
    "fig = plt.figure(layout='constrained')\n",
    "# Create the main Axes, leaving 25% of the figure space at the top and on the\n",
    "# right to position marginals.\n",
    "ax = fig.add_gridspec(top=0.75, right=0.75).subplots()\n",
    "# The main Axes' aspect can be fixed.\n",
    "ax.set(aspect=1)\n",
    "# Create marginal Axes, which have 25% of the size of the main Axes.  Note that\n",
    "# the inset Axes are positioned *outside* (on the right and the top) of the\n",
    "# main Axes, by specifying axes coordinates greater than 1.  Axes coordinates\n",
    "# less than 0 would likewise specify positions on the left and the bottom of\n",
    "# the main Axes.\n",
    "ax_histx = ax.inset_axes([0, 1.05, 1, 0.25], sharex=ax)\n",
    "ax_histy = ax.inset_axes([1.05, 0, 0.25, 1], sharey=ax)\n",
    "# Draw the scatter plot and marginals.\n",
    "scatter_hist(x, y, ax, ax_histx, ax_histy)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a99514-298b-46ac-930e-172fd0d86950",
   "metadata": {},
   "source": [
    "# Evotorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f69cdf-f58a-46f0-8e22-54dfa62ea33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evotorch import Problem\n",
    "from evotorch.algorithms import SNES\n",
    "from evotorch.logging import StdOutLogger\n",
    "\n",
    "def norm(x: torch.Tensor) -> torch.Tensor:\n",
    "  return torch.linalg.norm(x, dim=-1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    observed_rays = model(compensated_parameters_selected)\n",
    "uncompensated_parameters = uncompensated_parameters_selected\n",
    "def loss(x: torch.Tensor) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        tensor_sum = model.rescale_offset(x) + uncompensated_parameters\n",
    "        compensated_rays = model(tensor_sum, clone_output=True)\n",
    "        #compensated_rays = compensated_rays.flatten(start_dim=2)\n",
    "        loss_orig = ((compensated_rays - observed_rays) ** 2).mean(0).mean(0).mean(-1)\n",
    "        #loss_orig = ((compensated_rays - observed_rays) ** 2).mean(0).mean(-1)\n",
    "    return loss_orig\n",
    "\n",
    "problem = Problem(\n",
    "  \"min\",\n",
    "  loss,\n",
    "  initial_bounds=(0., 1.),\n",
    "  solution_length=model.mutable_parameter_count,\n",
    "  vectorized=True,\n",
    "  device=\"cuda\",  # Enable for GPU support\n",
    ")\n",
    "\n",
    "searcher = SNES(problem, popsize=1000, stdev_init=0.3)\n",
    "_ = StdOutLogger(searcher, interval=1000)\n",
    "\n",
    "searcher.run(num_generations=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abffe766-3b6c-476c-a4bf-d22bd55a9245",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_discovered_solution = searcher.status[\"pop_best\"]\n",
    "print(best_discovered_solution.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bef591-1594-40af-bd3c-e862a9603f56",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a98980-c87c-4bd4-a775-e92dc0062c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10000\n",
    "x = torch.rand((batch_size, uncompensated_parameters.shape[-1]), requires_grad=True, device=model.device)\n",
    "optimize_algo = 'adam'\n",
    "if optimize_algo == 'adam':\n",
    "    optimizer = torch.optim.Adam([x], lr=0.01)  # You can adjust the learning rate as needed\n",
    "else:\n",
    "    optimizer = torch.optim.SGD([x], lr=0.01)  # Adjust the learning rate as needed\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "num_epochs = 1000  # Number of iterations\n",
    "uncompensated_parameters = uncompensated_parameters_selected\n",
    "#with torch.no_grad():\n",
    "observed_rays = model(compensated_parameters_selected, clone_output=True, grad=True)\n",
    "\n",
    "def function_to_minimize(x):\n",
    "    tensor_sum = x + uncompensated_parameters\n",
    "    compensated_rays = model(tensor_sum, clone_output=True, grad=True)\n",
    "    compensated_rays = compensated_rays.flatten(start_dim=2)\n",
    "    loss_orig = ((compensated_rays - observed_rays) ** 2).mean(0).mean(-1)\n",
    "    return loss_orig\n",
    "\n",
    "pbar = trange(num_epochs)\n",
    "for epoch in pbar:\n",
    "    optimizer.zero_grad()  # Clear the gradients from the previous step\n",
    "    \n",
    "    output = function_to_minimize(x.detach())  # Compute the function's output\n",
    "    loss = output.mean()  # Compute the mean loss for this batch\n",
    "    \n",
    "    loss.backward(retain_graph=True)  # Backpropagate to compute the gradients\n",
    "    optimizer.step()  # Update the parameters with SGD\n",
    "\n",
    "    # Optionally print the loss to monitor progress\n",
    "    if epoch % 100 == 0:\n",
    "        pbar.set_postfix({\"loss\": loss.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004cddb9-a0f0-4524-9d4b-a329bc9aacce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(value, o1, o2, m1, m2):\n",
    "    if o2 == o1:\n",
    "        raise ValueError(\"The interval [x1, x2] cannot have zero width.\")\n",
    "    return m1 + (value - o1) * (m2 - m1) / (o2 - o1)\n",
    "\n",
    "# Example usage:\n",
    "o1, o2 = 0, 10  # original interval\n",
    "m1, m2 = 100, 200  # new interval\n",
    "value = 5  # value to rescale\n",
    "\n",
    "rescaled_value = rescale(value, o1, o2, m1, m2)\n",
    "print(\"Rescaled value:\", rescaled_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc33b03-173e-401d-9619-9c624da20180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(value, o1, o2, m1, m2):\n",
    "    rescale_scaler = 1. / (m2-m1)\n",
    "    rescale_multiplier = (o2-o1) * rescale_scaler\n",
    "    rescale_addend = (o1-m1) * rescale_scaler\n",
    "    return value*rescale_multiplier+rescale_addend\n",
    "    \n",
    "# Example usage:\n",
    "o1, o2 = -1, 1  # original interval\n",
    "m1, m2 = -3, 3  # new interval\n",
    "value = 0  # value to rescale\n",
    "\n",
    "rescaled_value = rescale(value, o1, o2, m1, m2)\n",
    "print(\"Rescaled value:\", rescaled_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e2e286-8c91-4e48-ae3e-9c8fc881d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0., 1., 2., 3.]\n",
    "torch.tensor(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cbc605-acff-412e-b14d-1c9fbed48397",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in model.offset_space.items():\n",
    "    if isinstance(value, MutableParameter):\n",
    "        print(key, value.value_lims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9427a92b-b5b9-42ea-aea4-6a23d9da42e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
